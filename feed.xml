<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <generator uri="http://jekyllrb.com" version="3.9.3">Jekyll</generator>
  
  
  <link href="https://dotkay.github.io/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://dotkay.github.io/" rel="alternate" type="text/html" hreflang="en" />
  <updated>2023-06-20T14:37:50+00:00</updated>
  <id>https://dotkay.github.io//</id>

  
    <title type="html">Kalyan</title>
  

  
    <subtitle>Learning by teaching</subtitle>
  

  

  
  
    <entry>
      
      <title type="html">Logical Reasoning and Knowledge Discovery →</title>
      
      <link href="https://dotkay.github.io/2023/03/28/knowledge-and-discovery/" rel="alternate" type="text/html" title="Logical Reasoning and Knowledge Discovery" />
      <published>2023-03-28T00:00:00+00:00</published>
      <updated>2023-03-28T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2023/03/28/knowledge-and-discovery</id>
      <content type="html" xml:base="https://dotkay.github.io/2023/03/28/knowledge-and-discovery/">&lt;p&gt;When a Deep Learning &lt;a href=&quot;https://news.mit.edu/2020/artificial-intelligence-identifies-new-antibiotic-0220&quot;&gt;algorithm invented Halicin&lt;/a&gt;, there was instant speculation about the ability of Deep Learning algorithms to discover new knowledge which in this case was a new antibiotic compound. This compound also seemed to have a new structure that was different from existing antibiotic compounds. Does this constitute new knowledge? Of course, it does. After all, the structure of the compound was not known earlier. This is akin to solving a Sudoku puzzle by searching through an enormous search space and arriving at a solution. The solution &lt;em&gt;is&lt;/em&gt; new knowledge - depending on the way you look at it.&lt;/p&gt;

&lt;p&gt;While AI systems today are more akin to &lt;a href=&quot;https://dotkay.github.io/2023/01/26/analogy-machines-reasoning&quot;&gt;&lt;em&gt;analogy machines&lt;/em&gt; than &lt;em&gt;reasoning machines&lt;/em&gt;&lt;/a&gt; there is an intense debate about what kind of a machine would lead us to our quest of Artificial General Intelligence (AGI) - is the ability to reason needed at all? After all, human beings ourselves learn more through experiences and we reason more from our past experiences than from the rule book. However, according to Stanislas Dehaene’s book &lt;a href=&quot;https://www.amazon.com/How-We-Learn-Brains-Machine/dp/0525559884&quot;&gt;&lt;em&gt;How We Learn&lt;/em&gt;&lt;/a&gt; we have certain intuition built into our brain that enables us to learn abstract concepts. In his book, he notes:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Characteristic of the human species is a relentless search for abstract rules, high-level conclusions that are extracted from a specific situation and subsequently tested on new observations 
…
Learning means managing an internal hierarchy of rules and trying to infer the most general ones that summarize a whole series of observations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This looks similar to logical reasoning (in the sense of mathematical logic in the realm of formal methods) where conclusions are inferred from hypotheses using a sequence of deduction rules. In order to illustrate this, let me pick one of the puzzles from &lt;a href=&quot;https://www.tptp.org/&quot;&gt;TPTP library&lt;/a&gt; - the &lt;em&gt;Aunt Agatha&lt;/em&gt; puzzle which is as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Someone who lives in Dreadbury Mansion killed Aunt Agatha. Agatha, the butler, and Charles live in Dreadbury Mansion, and are the only people who live therein. A killer always hates his victim, and is never richer than his victim. Charles hates no one that Aunt Agatha hates. Agatha hates everyone except the butler. The butler hates everyone not richer than Aunt Agatha. The butler hates everyone Aunt Agatha hates. No one hates everyone. Agatha is not the butler. Therefore: Agatha killed herself.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If one were to encode the given facts in mathematical logic and prove the conclusion (the conjecture that Agatha killed herself) is deducible from the given facts, say in a theorem prover (like Vampire), it would be able to not only prove the conjecture, the &lt;em&gt;logical reasoning system&lt;/em&gt; in Vampire would be able to show the sequence of steps that was used to arrive at the conclusion from the assumptions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;29. lives(sK0) [cnf transformation 26]
30. killed(sK0,agatha) [cnf transformation 26]
...
...
...
79. ~hates(butler,butler) [superposition 41,78]
...
...
82. hates(charles,agatha) &amp;lt;- (1) [backward demodulation 44,57]
89. ~hates(agatha,agatha) &amp;lt;- (1) [resolution 82,37]
...
...
93. $false [avatar sat refutation 66,71,81,92]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A Vampire proof run would also discover new knowledge (i.e. new facts not explicitly stated in our input set of facts). For example, it could deduce that the butler does not hate himself (line 79 above) which is not explicitly mentioned in the set of assumptions (derived from our natural natural language text). Such a kind of logical reasoning would be a great cornerstone for &lt;em&gt;explainable AI systems&lt;/em&gt; of the future. However, languages themselves are ambiguous and translating from an ambiguous natural language into an unambiguous formulas in first order logic (as in this case) is not a trivial task. Maybe, just maybe one could generate such formulas themselves by training a model using a large corpus of natual language text and the corresponding logical formulas that would be able to capture the essence of natual language statements, context, etc. in logical form.&lt;/p&gt;

&lt;p&gt;Note that the present day AI systems though demonstrate exemplary generative skills across text, images and video, the content generated, though novel in some cases, is an emergent behavior that arises from the vast knowledge base the model was trained on. Hence, it still lacks the capability to reason how it generated a particular content (it can not produce a sequence of deductions for example).&lt;/p&gt;

&lt;p&gt;A symbiotic combination of logical reasoning and analogy based reasoning would enable both an efficient generation of impressive content while at the same time being able to reason about the chain of thought involved in the process. &lt;a href=&quot;https://arxiv.org/abs/2201.08860&quot;&gt;GreaseLM&lt;/a&gt; and &lt;a href=&quot;https://proceedings.mlr.press/v139/ren21a.html&quot;&gt;LEGO&lt;/a&gt; are research efforts towards infusing reasoning capabilities in language models. They however work on pruning and traversing a knowledge graph based on the context from embedding vectors, rather than translating the knowledge into formulas in mathematical logic. AI systems that need to solve problems in mathematics and to derive &lt;em&gt;new knowledge&lt;/em&gt; in the form of new mathematical proofs or conjectures would need more rigour and encoding the knowledge of mathematics in the form of logical formulas and training AI systems on such a corpus could be one way of achieving reasoning capabilities. Besides solving competitive math problems, such systems might be able to discover new physical structures that are lighter and stronger, or better algorithms that are currently unknown, better deterministic optimal algorithms in the place of present day heuristics, or even proofs of non-existence of better algorithms for certain problems.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="AI/ML" />
      
        <category term="reasoning" />
      

      

      
        <summary type="html">When a Deep Learning algorithm invented Halicin, there was instant speculation about the ability of Deep Learning algorithms to discover new knowledge which in this case was a new antibiotic compound. This compound also seemed to have a new structure that was different from existing antibiotic compounds. Does this constitute new knowledge? Of course, it does. After all, the structure of the compound was not known earlier. This is akin to solving a Sudoku puzzle by searching through an enormous search space and arriving at a solution. The solution is new knowledge - depending on the way you look at it. While AI systems today are more akin to analogy machines than reasoning machines there is an intense debate about what kind of a machine would lead us to our quest of Artificial General Intelligence (AGI) - is the ability to reason needed at all? After all, human beings ourselves learn more through experiences and we reason more from our past experiences than from the rule book. However, according to Stanislas Dehaene’s book How We Learn we have certain intuition built into our brain that enables us to learn abstract concepts. In his book, he notes: Characteristic of the human species is a relentless search for abstract rules, high-level conclusions that are extracted from a specific situation and subsequently tested on new observations … Learning means managing an internal hierarchy of rules and trying to infer the most general ones that summarize a whole series of observations. This looks similar to logical reasoning (in the sense of mathematical logic in the realm of formal methods) where conclusions are inferred from hypotheses using a sequence of deduction rules. In order to illustrate this, let me pick one of the puzzles from TPTP library - the Aunt Agatha puzzle which is as follows: Someone who lives in Dreadbury Mansion killed Aunt Agatha. Agatha, the butler, and Charles live in Dreadbury Mansion, and are the only people who live therein. A killer always hates his victim, and is never richer than his victim. Charles hates no one that Aunt Agatha hates. Agatha hates everyone except the butler. The butler hates everyone not richer than Aunt Agatha. The butler hates everyone Aunt Agatha hates. No one hates everyone. Agatha is not the butler. Therefore: Agatha killed herself. If one were to encode the given facts in mathematical logic and prove the conclusion (the conjecture that Agatha killed herself) is deducible from the given facts, say in a theorem prover (like Vampire), it would be able to not only prove the conjecture, the logical reasoning system in Vampire would be able to show the sequence of steps that was used to arrive at the conclusion from the assumptions. 29. lives(sK0) [cnf transformation 26] 30. killed(sK0,agatha) [cnf transformation 26] ... ... ... 79. ~hates(butler,butler) [superposition 41,78] ... ... 82. hates(charles,agatha) &amp;lt;- (1) [backward demodulation 44,57] 89. ~hates(agatha,agatha) &amp;lt;- (1) [resolution 82,37] ... ... 93. $false [avatar sat refutation 66,71,81,92] A Vampire proof run would also discover new knowledge (i.e. new facts not explicitly stated in our input set of facts). For example, it could deduce that the butler does not hate himself (line 79 above) which is not explicitly mentioned in the set of assumptions (derived from our natural natural language text). Such a kind of logical reasoning would be a great cornerstone for explainable AI systems of the future. However, languages themselves are ambiguous and translating from an ambiguous natural language into an unambiguous formulas in first order logic (as in this case) is not a trivial task. Maybe, just maybe one could generate such formulas themselves by training a model using a large corpus of natual language text and the corresponding logical formulas that would be able to capture the essence of natual language statements, context, etc. in logical form. Note that the present day AI systems though demonstrate exemplary generative skills across text, images and video, the content generated, though novel in some cases, is an emergent behavior that arises from the vast knowledge base the model was trained on. Hence, it still lacks the capability to reason how it generated a particular content (it can not produce a sequence of deductions for example). A symbiotic combination of logical reasoning and analogy based reasoning would enable both an efficient generation of impressive content while at the same time being able to reason about the chain of thought involved in the process. GreaseLM and LEGO are research efforts towards infusing reasoning capabilities in language models. They however work on pruning and traversing a knowledge graph based on the context from embedding vectors, rather than translating the knowledge into formulas in mathematical logic. AI systems that need to solve problems in mathematics and to derive new knowledge in the form of new mathematical proofs or conjectures would need more rigour and encoding the knowledge of mathematics in the form of logical formulas and training AI systems on such a corpus could be one way of achieving reasoning capabilities. Besides solving competitive math problems, such systems might be able to discover new physical structures that are lighter and stronger, or better algorithms that are currently unknown, better deterministic optimal algorithms in the place of present day heuristics, or even proofs of non-existence of better algorithms for certain problems.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Analogy Machines or Reasoning Engines? →</title>
      
      <link href="https://dotkay.github.io/2023/01/26/analogy-machines-reasoning/" rel="alternate" type="text/html" title="Analogy Machines or Reasoning Engines?" />
      <published>2023-01-26T00:00:00+00:00</published>
      <updated>2023-01-26T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2023/01/26/analogy-machines-reasoning</id>
      <content type="html" xml:base="https://dotkay.github.io/2023/01/26/analogy-machines-reasoning/">&lt;p&gt;The foundations for reasoning about computer algorithms predate the development of computer programming languages themselves. In fact, &lt;em&gt;logic&lt;/em&gt; itself was developed in order to make arguments and discourse in natural language in a rigorous and structured manner so as to be able to reason about them. The word &lt;em&gt;logic&lt;/em&gt; itself originates from the Greek word &lt;em&gt;logos&lt;/em&gt; which could be loosely translated to &lt;em&gt;reason&lt;/em&gt;, &lt;em&gt;discourse&lt;/em&gt; or &lt;em&gt;language&lt;/em&gt; and it involves the study of &lt;em&gt;rules of thought&lt;/em&gt; or &lt;em&gt;rules of reasoning&lt;/em&gt;. This allowed to deduce conclusions from a set of premises in a logical and structured way, removing many of the ambiguities. For a long time however, logic (in particular mathematical logic) was used to advance the field of programming languages - in particular to reason about the correctness of algorithms (erstwhile AI - symbolic AI or symbolic reasoning) given the various data structures provided by the different programming languages. When you think about it the tool (&lt;em&gt;mathematical logic&lt;/em&gt;) that was developed to help in reasoning about natural languages made way to develop tools for more constrained language, with strict syntax and mostly strict semantics. In the last decade this has turned around and advances in algorithms, data and computation have enabled building systems that could understand natural language and attempt to reason about them.&lt;/p&gt;

&lt;p&gt;We see two approaches to automated artificial intelligence:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Reasoning Engines:&lt;/em&gt; Based on &lt;em&gt;rules&lt;/em&gt; or mathematical logic that allows one to reason about (give mathematical proofs) following rigorously established rules of mathematics and semantics of languages. This was the erstwhile AI that had several winters from 1950s to 1980s as data was expensive (there were no smartphone cameras) and computation was very expensive (there were no GPU data-centers).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Analogy Machines:&lt;/em&gt; Based on &lt;em&gt;patterns&lt;/em&gt; seen in large amounts of similar data. The rise of inexpensive sensors (cameras) and proliferation of computing (smartphones) and internet enabled vast amount of data to pattern match (i.e. learn) from.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Is &lt;em&gt;reasoning&lt;/em&gt; the core of intelligence as we usually think? Or would analogy machines turn out to be the &lt;em&gt;Artificial General Intelligence (AGI)&lt;/em&gt; that everyone is racing for? Or would it be a powerful system that is able to automate several tasks and develop new impactful discoveries for mankind - like &lt;a href=&quot;https://news.mit.edu/2020/artificial-intelligence-identifies-new-antibiotic-0220&quot;&gt;Halicin&lt;/a&gt;, but still not able to deduce everything that human minds are capable of? There is an intense ongoing debate between Computer Scientists, Philosophers, Neuroscientists, medical doctors, religious leaders and of course the general public about if the magnificient advancements in &lt;em&gt;analogy machines&lt;/em&gt;, henceforth Deep Learning, one day lead to true AGI and what would positive and negative impacts of such a system be.&lt;/p&gt;

&lt;p&gt;In a recent interview, Prof. Geoffrey Hinton a Turing award winning Computer Scientist claims that &lt;em&gt;reasoning&lt;/em&gt; may not be the core of intelligence and that there are things similar to &lt;em&gt;vectors&lt;/em&gt; in the brain that are just patterns of activities that interact with one another. This is similar to &lt;em&gt;word embeddings&lt;/em&gt; in large language models (LLMs). There is a fantastic study about holographic memory by &lt;a href=&quot;https://psychology.fas.harvard.edu/people/karl-lashley&quot;&gt;Prof. Karl Lashley&lt;/a&gt; who experimented with rats’ behavior in mazes and concluded that &lt;em&gt;all&lt;/em&gt; information is stored &lt;em&gt;everywhere&lt;/em&gt; in a distributed fashion and even if one part of the memory gets damaged one has access to &lt;em&gt;all&lt;/em&gt; of the information.&lt;/p&gt;

&lt;p&gt;Prof. Hinton contrasted symbolic reasoning and analogical reasoning and claimed that reasoning is easy when done using symbolic reasoning but perception is hard. It is hard to encode what we perceive into logical statements without introducing ambiguity. He claims that the reasoning employed by human beings is almost always &lt;em&gt;analogical reasoning&lt;/em&gt; - reasoning by making analogies. Languages themselves are abstractions - they are just an approximate encoding for communicating our thoughts and not the actual representation of our thoughts. Hence different human beings might convey the same thought in different ways. The languages themselves as employed by us are just pointers to the vectors in our brain (similar to the weights in LLMs  that point to embedding vectors).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is quite likely we will get some kind of symbiosis. AI will make us far more competent. It is changing our view of what we are … it is changing people’s view from the idea that the essence of a person is a deliberate reasoning machine; the essence is much more a huge analogy machine … that seems far more like our real nature, than reasoning machines   – Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There used to be research interest in identifying which language constructs would make the best compiler or the best programming language and Computer scientists have looked at languages like &lt;a href=&quot;http://gallium.inria.fr/~huet/PUBLIC/Panini_machine.pdf&quot;&gt;Sanskrit&lt;/a&gt; to and deconstruct &lt;a href=&quot;https://bhavana.org.in/computing-processes-and-pan%CC%A3inis-as%CC%A3t%CC%A3adhyayi/&quot;&gt;Panini’s rules&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, languages themselves are not the same as are human thoughts and expressions of thoughts. Some languages have ways of expressing things that certain other languages do not. Some people have ways of articulating their thoughts that many others do not. Given this large variance, I am waiting to see how long before LLMs can become a more effective teacher or a more effective comedian.&lt;/p&gt;

&lt;p&gt;I am also waiting to see what would be the best language for a machine to express thoughts would be - would it even be a textual language or would it be a visual one? Conversely what would be the best mode to communicate with an analogy machine be? How would you reason about what analogies it used to arrive at a prediction and in what sequence or order it used the analogies - much like a deductive reasoning engine for programming languages.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="AI/ML" />
      
        <category term="reasoning" />
      

      

      
        <summary type="html">The foundations for reasoning about computer algorithms predate the development of computer programming languages themselves. In fact, logic itself was developed in order to make arguments and discourse in natural language in a rigorous and structured manner so as to be able to reason about them. The word logic itself originates from the Greek word logos which could be loosely translated to reason, discourse or language and it involves the study of rules of thought or rules of reasoning. This allowed to deduce conclusions from a set of premises in a logical and structured way, removing many of the ambiguities. For a long time however, logic (in particular mathematical logic) was used to advance the field of programming languages - in particular to reason about the correctness of algorithms (erstwhile AI - symbolic AI or symbolic reasoning) given the various data structures provided by the different programming languages. When you think about it the tool (mathematical logic) that was developed to help in reasoning about natural languages made way to develop tools for more constrained language, with strict syntax and mostly strict semantics. In the last decade this has turned around and advances in algorithms, data and computation have enabled building systems that could understand natural language and attempt to reason about them. We see two approaches to automated artificial intelligence: Reasoning Engines: Based on rules or mathematical logic that allows one to reason about (give mathematical proofs) following rigorously established rules of mathematics and semantics of languages. This was the erstwhile AI that had several winters from 1950s to 1980s as data was expensive (there were no smartphone cameras) and computation was very expensive (there were no GPU data-centers). Analogy Machines: Based on patterns seen in large amounts of similar data. The rise of inexpensive sensors (cameras) and proliferation of computing (smartphones) and internet enabled vast amount of data to pattern match (i.e. learn) from. Is reasoning the core of intelligence as we usually think? Or would analogy machines turn out to be the Artificial General Intelligence (AGI) that everyone is racing for? Or would it be a powerful system that is able to automate several tasks and develop new impactful discoveries for mankind - like Halicin, but still not able to deduce everything that human minds are capable of? There is an intense ongoing debate between Computer Scientists, Philosophers, Neuroscientists, medical doctors, religious leaders and of course the general public about if the magnificient advancements in analogy machines, henceforth Deep Learning, one day lead to true AGI and what would positive and negative impacts of such a system be. In a recent interview, Prof. Geoffrey Hinton a Turing award winning Computer Scientist claims that reasoning may not be the core of intelligence and that there are things similar to vectors in the brain that are just patterns of activities that interact with one another. This is similar to word embeddings in large language models (LLMs). There is a fantastic study about holographic memory by Prof. Karl Lashley who experimented with rats’ behavior in mazes and concluded that all information is stored everywhere in a distributed fashion and even if one part of the memory gets damaged one has access to all of the information. Prof. Hinton contrasted symbolic reasoning and analogical reasoning and claimed that reasoning is easy when done using symbolic reasoning but perception is hard. It is hard to encode what we perceive into logical statements without introducing ambiguity. He claims that the reasoning employed by human beings is almost always analogical reasoning - reasoning by making analogies. Languages themselves are abstractions - they are just an approximate encoding for communicating our thoughts and not the actual representation of our thoughts. Hence different human beings might convey the same thought in different ways. The languages themselves as employed by us are just pointers to the vectors in our brain (similar to the weights in LLMs that point to embedding vectors). It is quite likely we will get some kind of symbiosis. AI will make us far more competent. It is changing our view of what we are … it is changing people’s view from the idea that the essence of a person is a deliberate reasoning machine; the essence is much more a huge analogy machine … that seems far more like our real nature, than reasoning machines – Geoffrey Hinton There used to be research interest in identifying which language constructs would make the best compiler or the best programming language and Computer scientists have looked at languages like Sanskrit to and deconstruct Panini’s rules. However, languages themselves are not the same as are human thoughts and expressions of thoughts. Some languages have ways of expressing things that certain other languages do not. Some people have ways of articulating their thoughts that many others do not. Given this large variance, I am waiting to see how long before LLMs can become a more effective teacher or a more effective comedian. I am also waiting to see what would be the best language for a machine to express thoughts would be - would it even be a textual language or would it be a visual one? Conversely what would be the best mode to communicate with an analogy machine be? How would you reason about what analogies it used to arrive at a prediction and in what sequence or order it used the analogies - much like a deductive reasoning engine for programming languages.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">From Feature Engineering to Prompt Engineering →</title>
      
      <link href="https://dotkay.github.io/2022/12/21/feature-prompt-engg/" rel="alternate" type="text/html" title="From Feature Engineering to Prompt Engineering" />
      <published>2022-12-21T00:00:00+00:00</published>
      <updated>2022-12-21T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2022/12/21/feature-prompt-engg</id>
      <content type="html" xml:base="https://dotkay.github.io/2022/12/21/feature-prompt-engg/">&lt;p&gt;A few years ago, when Machine Learning was at its infancy, &lt;em&gt;feature engineering&lt;/em&gt; used to be the swiss army knife for getting great results out of the models. Many &lt;em&gt;domain experts&lt;/em&gt;, who claimed to know which features were important and which features weren’t important for a particular problem, became widely sought after. From a handful of features for predicting housing prices, like number of rooms, square footage, Zipcode, etc., to several hundreds of millions of features that might be present in a typical social media feed recommendation system, model features have grown at an impressive rate, thanks to the many varieties of inexpensive sensors and of course the distribution power of the internet. &lt;em&gt;Feature Engineering&lt;/em&gt; became a role on its own in many companies that were building machine learning models. An automobile engineer would know which features are relevant or important to the stability of a car, while a grocery retailer would know about which items when placed at specific aisles in the store would garner more interest. Unsurprisingly, this also led to considerable interest from academia and subsequently there were more efficient techniques formalized and developed for effective &lt;em&gt;feature selection&lt;/em&gt; based on the problem at hand and &lt;em&gt;feature construction&lt;/em&gt; and &lt;em&gt;feature extraction&lt;/em&gt; from raw data. Engineering teams started developing tools that facilitated such processes - like &lt;a href=&quot;https://www.featuretools.com/&quot;&gt;Featuretools&lt;/a&gt; for Python.&lt;/p&gt;

&lt;p&gt;Fast forward a decade, feature engineering seems to be less in fashion while &lt;em&gt;prompt engineering&lt;/em&gt; has captured the podium. This is primarily due to the interest and advancement in large language models. While earlier machine learning models were built to solve specific problems and hence needed domain experts to curate the features that best helps the model, advancements in few shot learning made selecting specific features less and less relevant while maintaining the same accuracy of the model. Today, models like OpenAI’s ChatGPT and the like can work on a variety of domains as they are trained on a collective dataset (dataset from as many different domains as possible) and hence have captured the relationships between the different domains by attaching weights to the learned contexts as did the previous machine learning algorithms that attached weights to different features. A word like &lt;em&gt;teddy&lt;/em&gt; would be identified with a context of &lt;em&gt;toys&lt;/em&gt; more (larger weights) while the data is about donations to charity for helping children. The same word however would be identified with a context of &lt;em&gt;politics&lt;/em&gt; while the data is about national parks and national forests.&lt;/p&gt;

&lt;p&gt;For example, experimenting with &lt;a href=&quot;https://openai.com/blog/chatgpt&quot;&gt;ChatGPT&lt;/a&gt;, when the system is prompted with the following question, it prompts back asking for clarifications, in order to identify the context.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user: Can you tell me about teddy?
ChatGPT: I'm sorry, but without additional context, I'm not sure which &quot;teddy&quot; you're referring to. 
There are several possible meanings for the word &quot;teddy,&quot; such as:
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The user might give an indirect context that the system still identifies with the initial prompt (&lt;em&gt;teddy&lt;/em&gt;) - i.e. the system has memory and it retrieves the weights from its collection of different contexts, much like an indexing system would retrieve the best page among a collection of pages related to a search keyword.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user: tell me about the teddy related to national parks and forests
ChatGPT: Theodore Roosevelt is closely associated with the conservation movement in the United States and is often referred 
to as the &quot;Conservation President.&quot; During his presidency from 1901 to 1909, he took significant steps to protect natural 
resources, establish national parks, and preserve wilderness areas.
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The user could still prompt the system with a subsequent string that could entirely change the course of the conversation, much like how many real-world informal conversations move around from one topic to another while the topics themselves might be unrelated but tied together by some weak links.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user: tell me about the teddy related to donations to charity for children
ChatGPT: One possible &quot;teddy&quot; you might be referring to in this context is the &quot;Teddy Bear&quot; charity program. 
The Teddy Bear program is a charitable initiative that collects and distributes teddy bears and other stuffed animals 
to children in need, such as those who are sick, homeless, or traumatized.
The Teddy Bear program was first started by police officers in the United States in ...
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The user could ask about an unrelated topic as a subsequent prompt but the system might still try to find a common context and if there is would talk about it, because the subsequent prompts are considered part of the same conversation.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user: what about fashion?
ChatGPT: In the world of fashion, a &quot;teddy&quot; can refer to a type of lingerie for women. A teddy is a 
one-piece garment that combines a camisole top and panties, often made from silk or other
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why is this impressive or important. In many real-world impactful applications (rather than informal chat systems), it is crucial to maintain a conversation topic - think about an airline customer service chatbot or an internet service center or education software where one builds on from simple fundamental concepts to advanced topics that are built by stitching together fundamental concepts. It is also crucial to have the ability to switch to a completely different topic even when there might be a related weak link in the conversation thread. This switch is effected by engineering the prompt in such a way as to turn the system’s attention from it’s current context to a new context.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user: No, let's just talk about contemporary fashion...
ChatGPT: Certainly! Contemporary fashion is a constantly evolving and diverse industry, encompassing a 
wide range of styles, trends, and designers. Some of the key trends and movements in ...
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that &lt;em&gt;prompt engineering&lt;/em&gt; gets the job done, is &lt;em&gt;feature engineering&lt;/em&gt; irrelevant? Not really. It would still require a domain expert to help train the system with well engineered prompts in order for the system not to stray away. For example, a teacher could help with engineering the right prompts that would make the learning process more effective by not only charting the path to learn - from essential fundamental concepts to more involved concepts, but also in personalizing the path by identifying the learning challenges based on the questions (prompts) from the user (learner). This could be the next revolution in personalized, adaptive learning systems. There could be many other situations where such systems could benefit from domain experts. Some of them include negotiation systems, interviews and management training systems, navigation, technical support, as well as entertainment systems. Adding to this, multi-modal prompting systems, where the prompts themselves would have other modes (like voice, images, etc.) in addition to text would be immensely powerful in many real-world applications. Voice prompts would be very useful for autonomous machines, and image prompts would be exciting for content creation. I am eagerly looking forward to playing with those.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="AI/ML" />
      
        <category term="Feature Engineering" />
      
        <category term="Prompt" />
      
        <category term="OpenAI" />
      

      

      
        <summary type="html">A few years ago, when Machine Learning was at its infancy, feature engineering used to be the swiss army knife for getting great results out of the models. Many domain experts, who claimed to know which features were important and which features weren’t important for a particular problem, became widely sought after. From a handful of features for predicting housing prices, like number of rooms, square footage, Zipcode, etc., to several hundreds of millions of features that might be present in a typical social media feed recommendation system, model features have grown at an impressive rate, thanks to the many varieties of inexpensive sensors and of course the distribution power of the internet. Feature Engineering became a role on its own in many companies that were building machine learning models. An automobile engineer would know which features are relevant or important to the stability of a car, while a grocery retailer would know about which items when placed at specific aisles in the store would garner more interest. Unsurprisingly, this also led to considerable interest from academia and subsequently there were more efficient techniques formalized and developed for effective feature selection based on the problem at hand and feature construction and feature extraction from raw data. Engineering teams started developing tools that facilitated such processes - like Featuretools for Python. Fast forward a decade, feature engineering seems to be less in fashion while prompt engineering has captured the podium. This is primarily due to the interest and advancement in large language models. While earlier machine learning models were built to solve specific problems and hence needed domain experts to curate the features that best helps the model, advancements in few shot learning made selecting specific features less and less relevant while maintaining the same accuracy of the model. Today, models like OpenAI’s ChatGPT and the like can work on a variety of domains as they are trained on a collective dataset (dataset from as many different domains as possible) and hence have captured the relationships between the different domains by attaching weights to the learned contexts as did the previous machine learning algorithms that attached weights to different features. A word like teddy would be identified with a context of toys more (larger weights) while the data is about donations to charity for helping children. The same word however would be identified with a context of politics while the data is about national parks and national forests. For example, experimenting with ChatGPT, when the system is prompted with the following question, it prompts back asking for clarifications, in order to identify the context. user: Can you tell me about teddy? ChatGPT: I'm sorry, but without additional context, I'm not sure which &quot;teddy&quot; you're referring to. There are several possible meanings for the word &quot;teddy,&quot; such as: ... ... The user might give an indirect context that the system still identifies with the initial prompt (teddy) - i.e. the system has memory and it retrieves the weights from its collection of different contexts, much like an indexing system would retrieve the best page among a collection of pages related to a search keyword. user: tell me about the teddy related to national parks and forests ChatGPT: Theodore Roosevelt is closely associated with the conservation movement in the United States and is often referred to as the &quot;Conservation President.&quot; During his presidency from 1901 to 1909, he took significant steps to protect natural resources, establish national parks, and preserve wilderness areas. ... ... The user could still prompt the system with a subsequent string that could entirely change the course of the conversation, much like how many real-world informal conversations move around from one topic to another while the topics themselves might be unrelated but tied together by some weak links. user: tell me about the teddy related to donations to charity for children ChatGPT: One possible &quot;teddy&quot; you might be referring to in this context is the &quot;Teddy Bear&quot; charity program. The Teddy Bear program is a charitable initiative that collects and distributes teddy bears and other stuffed animals to children in need, such as those who are sick, homeless, or traumatized. The Teddy Bear program was first started by police officers in the United States in ... ... ... The user could ask about an unrelated topic as a subsequent prompt but the system might still try to find a common context and if there is would talk about it, because the subsequent prompts are considered part of the same conversation. user: what about fashion? ChatGPT: In the world of fashion, a &quot;teddy&quot; can refer to a type of lingerie for women. A teddy is a one-piece garment that combines a camisole top and panties, often made from silk or other ... ... Why is this impressive or important. In many real-world impactful applications (rather than informal chat systems), it is crucial to maintain a conversation topic - think about an airline customer service chatbot or an internet service center or education software where one builds on from simple fundamental concepts to advanced topics that are built by stitching together fundamental concepts. It is also crucial to have the ability to switch to a completely different topic even when there might be a related weak link in the conversation thread. This switch is effected by engineering the prompt in such a way as to turn the system’s attention from it’s current context to a new context. user: No, let's just talk about contemporary fashion... ChatGPT: Certainly! Contemporary fashion is a constantly evolving and diverse industry, encompassing a wide range of styles, trends, and designers. Some of the key trends and movements in ... ... ... Now that prompt engineering gets the job done, is feature engineering irrelevant? Not really. It would still require a domain expert to help train the system with well engineered prompts in order for the system not to stray away. For example, a teacher could help with engineering the right prompts that would make the learning process more effective by not only charting the path to learn - from essential fundamental concepts to more involved concepts, but also in personalizing the path by identifying the learning challenges based on the questions (prompts) from the user (learner). This could be the next revolution in personalized, adaptive learning systems. There could be many other situations where such systems could benefit from domain experts. Some of them include negotiation systems, interviews and management training systems, navigation, technical support, as well as entertainment systems. Adding to this, multi-modal prompting systems, where the prompts themselves would have other modes (like voice, images, etc.) in addition to text would be immensely powerful in many real-world applications. Voice prompts would be very useful for autonomous machines, and image prompts would be exciting for content creation. I am eagerly looking forward to playing with those.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">The Different Layers of AI Innovation →</title>
      
      <link href="https://dotkay.github.io/2022/08/17/ai-innovation-layers/" rel="alternate" type="text/html" title="The Different Layers of AI Innovation" />
      <published>2022-08-17T00:00:00+00:00</published>
      <updated>2022-08-17T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2022/08/17/ai-innovation-layers</id>
      <content type="html" xml:base="https://dotkay.github.io/2022/08/17/ai-innovation-layers/">&lt;p&gt;When we hear about AI applications these days, either companies like NVIDIA or Graphcore is in focus or the companies that develop applications like OpenAI or Meta is in focus. However there are several layers to AI. Understanding what constitutes the different layers and which companies are the key players in any or many of the layers would enable one to identify moats, challenges and growth drivers of such companies. Besides, one could also identify potential for future opportunities for vertical integration and consolidation.&lt;/p&gt;

&lt;p&gt;AI had many winters over the last several decades. However, over the last decade AI enabled applications seem to be proliferating and has already been infused with several day to day software applications - like voice assistants, maps, translators, etc. Two key developments that happened over the last couple of decades are chiefly responsible for this proliferation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Availability of data, due to inexpensive sensors (image sensors like camera, messages in social media apps, etc.)&lt;/li&gt;
  &lt;li&gt;Availability of compute, due to faster CPUs and GPUs&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Looking at just the two key components would mislead us to think about just software to process the data and hardware to execute the software. One might want to break it down into further components in order to identify the key players in each of the components.&lt;/p&gt;

&lt;h2&gt;Layers of AI&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/dotkay.github.io/source/assets/images/misc/ai_layers.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3&gt;Hardware Accelerator Layer&lt;/h3&gt;

&lt;p&gt;This is the one of the main layers that brought AI research out of its many winters. &lt;a href=&quot;https://inria.hal.science/inria-00112631/document&quot;&gt;Kumar’s work&lt;/a&gt; at Microsoft Research, &lt;a href=&quot;https://people.idsia.ch/~juergen/ijcai2011.pdf&quot;&gt;Dan Ciresan’s work&lt;/a&gt; at IDSIA on accelerating CNNs (Convolutional Neural Networks) using GPUs followed by the famous &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;&gt;AlexNet work&lt;/a&gt; brought attention to the use of hardware acclerators like GPUs for applications that are heavy on matrix computations. GPUs are Single Instruction Multiple Data (SIMD) architecture machines that could excel at tasks that are massively parallel. Subsequently, there were other accelerators built based on other architectures like Bulk Synchronous Processing (BSP) (e.g. Graphcore), or Systolic Arrays (e.g. TPUs) that were targeted on accelerating specific architectures. A broad install base as well as accelerators fine tuned over a long period of time to excel at specific deep learning model architectures and deep learning frameworks could serve as a moat that prevents others from having a slice of the market share easily. A good analogy is to think of hardware layer as the muscles that enable heavy computations.&lt;/p&gt;

&lt;h3&gt;System Software/Compiler Layer&lt;/h3&gt;

&lt;p&gt;The System Software or Compiler Layer sits closest to the hardware accelerator thereby optimizing the executions of machine instructions as much as possible. CUDA and OpenCL are the programming models closely tied to the software drivers (system software) or compilers. Lately, Graphcore has graph compilers that optimizes the execution of graph networks (using the Poplar SDK) on Graphcore IPUs. There are also companies like Groq that make a much tighter integration between the compiler layer and the hardware layer. In particular, the Groq compiler does all the optimizations on the input model and continuously orchestrates the instructions as operands for those instructions arrive, with the exact knowledge of which hardware execution units are available to execute the instructions.&lt;/p&gt;

&lt;h3&gt;Model Architecture Layer&lt;/h3&gt;

&lt;p&gt;It is not often that one can separate out this layer. Once certain model architectures become stabilized, it might make sense to optimize the layers beneath it (hardware and system software layers) in order to serve this particular architecture. The popular architectures might be Transformers for languages, Convolutional Neural Networks for Images, Graph Neural Networks for graph datasets, etc. However, since newer algorithms are still being developed, the current trends are more about optimizing the hardware and system software layers to optimize specific components of the models. The &lt;a href=&quot;https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/&quot;&gt;Transformer Engine&lt;/a&gt; in H100 GPUs would be an example of such an optimization.&lt;/p&gt;

&lt;h3&gt;Deep Learning (DL) Frameworks Layer&lt;/h3&gt;

&lt;p&gt;The different DL Frameworks like Tensorflow, PyTorch, JAX, etc. are similar superficially and have over the years become more similar than different. However, there are compiler instrumentations that optimize such frameworks, very much like Groq Compiler, NVIDIA CUDA, OpenAI Triton, etc. Some of the optimizations like operator fusion have been adopted by the DL frameworks like PyTorch by default and many more such optimizations might become default as research and engineering in DL advances. This layer is also important as it might turn out to be critical to the iterative development of algorithms. Most DL algorithms are iterative in nature and having fine grained control over the progress of the experiments in terms of debugging, performance analysis and the like would ultimately determine the popularity, adoption and subsequently advances in such frameworks. For example, PyTorch executes eagerly using dynamic graphs, it was much easier for programmers to monitor. Tensorflow moved to eager execution from later versions. Which framework is best suited is an ongoing debate in the community with academic researchers leaning towards PyTorch and production engineers leaning towards Tensorflow. There have been other frameworks which were popular initially but lost steam over time.&lt;/p&gt;

&lt;h3&gt;Algorithms Layer&lt;/h3&gt;

&lt;p&gt;Algorithmic innovations have led to developments in newer, efficient and more effective model architectures. Though some of the algorithms have been known for a very long time (like backpropagation, CNNs, etc.) the development in other layers (availability of general purpose programmable hardware accelerators, easy-to-use DL frameworks and most importantly the availability of data to train the networks) have significantly aided advancements in algorithms. 
This has led to architectures like &lt;em&gt;Transformers&lt;/em&gt; with layers like &lt;em&gt;Attention&lt;/em&gt; that seem to push forward the state-of-the-art in DL research. However, it wasn’t an easy innovation and it is definitely not the last - researchers tried different architectures from Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), Sequence-to-Sequence models, etc. before iteratively discovering the bottlenecks and devising newer architectures to optimize those. If one of these algorithms were determined to be the most optimal, it would make sense to bake it in silicon in order to get the most efficient performance. It has not happened yet as one does not know what future architectures would bring in this fast paced world of DL research.&lt;/p&gt;

&lt;h3&gt;Data Layer&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;There were 5 exabytes of information created between the dawn of civilization through 2003, but that much information is now created every 2 days.”  – &lt;em&gt;Eric Schmidt&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Data&lt;/em&gt; along with &lt;em&gt;Compute&lt;/em&gt; is one of the key drivers of today’s Deep Learning advances. The growth and availability of inexpensive sensors and access to analyze them has put all the algorithms that were developed in theory over the past several decades into practice within a span of a decade. However, as DL applications impermeate more and more of our lives, there would be questions around the source of &lt;em&gt;data&lt;/em&gt; the models were trained on and if the data provider was compensated for the same. Further, the emergent behaviors of today’s AI systems are the result of the data they were trained on but without having the ability to trace back the emergent behavior to the data points that led to it. These would be the areas of research that could eventually lead to &lt;em&gt;explainable AI&lt;/em&gt; systems that we aspire for.&lt;/p&gt;

&lt;h3&gt;Reasoning Layer&lt;/h3&gt;

&lt;p&gt;Different from logical reasoning systems which were referred to as &lt;em&gt;reasoning engines&lt;/em&gt;, today’s AI systems are more &lt;em&gt;analogy engines&lt;/em&gt; than reasoning ones. The exceptional efficiency and effectiveness of such systems seem to be the result of emergent behavior arising from training these systems on unbelievably vast quantities of data. Even with its effectiveness in language tasks, such AI systems seem to be not so good at logical reasoning. The infusion of logical reasoning behaviors into analogy based reasoning architectures of today’s systems would possibly create effective systems that is also able to reason about its emergent behaviors. Since the research in this area is not very well developed at the time of this writing, it is depicted as a grey box in the image above.&lt;/p&gt;

&lt;h3&gt;Regulatory Layer&lt;/h3&gt;

&lt;p&gt;This layer is still an underdeveloped layer (and hence depicted using dotted lines). With recent discussions around the data the systems are trained on and the transparency of algorithms and their explainability, this layer might encompass more than just the data layer and the algorithms layer. In the coming years the systems are likely to get regulated in order to deter inappropriate usage of such systems.&lt;/p&gt;

&lt;p&gt;It is indeed a great time for AI - the recent advances in research, several applications and multitude of challenges will make the next few years something exciting to look forward to.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="AI/ML" />
      
        <category term="DL Accelerators" />
      
        <category term="Frameworks" />
      
        <category term="Algorithms" />
      

      

      
        <summary type="html">When we hear about AI applications these days, either companies like NVIDIA or Graphcore is in focus or the companies that develop applications like OpenAI or Meta is in focus. However there are several layers to AI. Understanding what constitutes the different layers and which companies are the key players in any or many of the layers would enable one to identify moats, challenges and growth drivers of such companies. Besides, one could also identify potential for future opportunities for vertical integration and consolidation. AI had many winters over the last several decades. However, over the last decade AI enabled applications seem to be proliferating and has already been infused with several day to day software applications - like voice assistants, maps, translators, etc. Two key developments that happened over the last couple of decades are chiefly responsible for this proliferation: Availability of data, due to inexpensive sensors (image sensors like camera, messages in social media apps, etc.) Availability of compute, due to faster CPUs and GPUs Looking at just the two key components would mislead us to think about just software to process the data and hardware to execute the software. One might want to break it down into further components in order to identify the key players in each of the components. Layers of AI Hardware Accelerator Layer This is the one of the main layers that brought AI research out of its many winters. Kumar’s work at Microsoft Research, Dan Ciresan’s work at IDSIA on accelerating CNNs (Convolutional Neural Networks) using GPUs followed by the famous AlexNet work brought attention to the use of hardware acclerators like GPUs for applications that are heavy on matrix computations. GPUs are Single Instruction Multiple Data (SIMD) architecture machines that could excel at tasks that are massively parallel. Subsequently, there were other accelerators built based on other architectures like Bulk Synchronous Processing (BSP) (e.g. Graphcore), or Systolic Arrays (e.g. TPUs) that were targeted on accelerating specific architectures. A broad install base as well as accelerators fine tuned over a long period of time to excel at specific deep learning model architectures and deep learning frameworks could serve as a moat that prevents others from having a slice of the market share easily. A good analogy is to think of hardware layer as the muscles that enable heavy computations. System Software/Compiler Layer The System Software or Compiler Layer sits closest to the hardware accelerator thereby optimizing the executions of machine instructions as much as possible. CUDA and OpenCL are the programming models closely tied to the software drivers (system software) or compilers. Lately, Graphcore has graph compilers that optimizes the execution of graph networks (using the Poplar SDK) on Graphcore IPUs. There are also companies like Groq that make a much tighter integration between the compiler layer and the hardware layer. In particular, the Groq compiler does all the optimizations on the input model and continuously orchestrates the instructions as operands for those instructions arrive, with the exact knowledge of which hardware execution units are available to execute the instructions. Model Architecture Layer It is not often that one can separate out this layer. Once certain model architectures become stabilized, it might make sense to optimize the layers beneath it (hardware and system software layers) in order to serve this particular architecture. The popular architectures might be Transformers for languages, Convolutional Neural Networks for Images, Graph Neural Networks for graph datasets, etc. However, since newer algorithms are still being developed, the current trends are more about optimizing the hardware and system software layers to optimize specific components of the models. The Transformer Engine in H100 GPUs would be an example of such an optimization. Deep Learning (DL) Frameworks Layer The different DL Frameworks like Tensorflow, PyTorch, JAX, etc. are similar superficially and have over the years become more similar than different. However, there are compiler instrumentations that optimize such frameworks, very much like Groq Compiler, NVIDIA CUDA, OpenAI Triton, etc. Some of the optimizations like operator fusion have been adopted by the DL frameworks like PyTorch by default and many more such optimizations might become default as research and engineering in DL advances. This layer is also important as it might turn out to be critical to the iterative development of algorithms. Most DL algorithms are iterative in nature and having fine grained control over the progress of the experiments in terms of debugging, performance analysis and the like would ultimately determine the popularity, adoption and subsequently advances in such frameworks. For example, PyTorch executes eagerly using dynamic graphs, it was much easier for programmers to monitor. Tensorflow moved to eager execution from later versions. Which framework is best suited is an ongoing debate in the community with academic researchers leaning towards PyTorch and production engineers leaning towards Tensorflow. There have been other frameworks which were popular initially but lost steam over time. Algorithms Layer Algorithmic innovations have led to developments in newer, efficient and more effective model architectures. Though some of the algorithms have been known for a very long time (like backpropagation, CNNs, etc.) the development in other layers (availability of general purpose programmable hardware accelerators, easy-to-use DL frameworks and most importantly the availability of data to train the networks) have significantly aided advancements in algorithms. This has led to architectures like Transformers with layers like Attention that seem to push forward the state-of-the-art in DL research. However, it wasn’t an easy innovation and it is definitely not the last - researchers tried different architectures from Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), Sequence-to-Sequence models, etc. before iteratively discovering the bottlenecks and devising newer architectures to optimize those. If one of these algorithms were determined to be the most optimal, it would make sense to bake it in silicon in order to get the most efficient performance. It has not happened yet as one does not know what future architectures would bring in this fast paced world of DL research. Data Layer There were 5 exabytes of information created between the dawn of civilization through 2003, but that much information is now created every 2 days.” – Eric Schmidt Data along with Compute is one of the key drivers of today’s Deep Learning advances. The growth and availability of inexpensive sensors and access to analyze them has put all the algorithms that were developed in theory over the past several decades into practice within a span of a decade. However, as DL applications impermeate more and more of our lives, there would be questions around the source of data the models were trained on and if the data provider was compensated for the same. Further, the emergent behaviors of today’s AI systems are the result of the data they were trained on but without having the ability to trace back the emergent behavior to the data points that led to it. These would be the areas of research that could eventually lead to explainable AI systems that we aspire for. Reasoning Layer Different from logical reasoning systems which were referred to as reasoning engines, today’s AI systems are more analogy engines than reasoning ones. The exceptional efficiency and effectiveness of such systems seem to be the result of emergent behavior arising from training these systems on unbelievably vast quantities of data. Even with its effectiveness in language tasks, such AI systems seem to be not so good at logical reasoning. The infusion of logical reasoning behaviors into analogy based reasoning architectures of today’s systems would possibly create effective systems that is also able to reason about its emergent behaviors. Since the research in this area is not very well developed at the time of this writing, it is depicted as a grey box in the image above. Regulatory Layer This layer is still an underdeveloped layer (and hence depicted using dotted lines). With recent discussions around the data the systems are trained on and the transparency of algorithms and their explainability, this layer might encompass more than just the data layer and the algorithms layer. In the coming years the systems are likely to get regulated in order to deter inappropriate usage of such systems. It is indeed a great time for AI - the recent advances in research, several applications and multitude of challenges will make the next few years something exciting to look forward to.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Logic, Languages and Reasoning →</title>
      
      <link href="https://dotkay.github.io/2022/04/24/logic-languages-reasoning/" rel="alternate" type="text/html" title="Logic, Languages and Reasoning" />
      <published>2022-04-24T00:00:00+00:00</published>
      <updated>2022-04-24T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2022/04/24/logic-languages-reasoning</id>
      <content type="html" xml:base="https://dotkay.github.io/2022/04/24/logic-languages-reasoning/">&lt;p&gt;Human languages developed and evolved over thousands of years. Though most languages have a structure (defined by the rules of grammar) we think of spoken languages as a series of informal statements (“informal” compared to the “formal” rigor of mathematical statements). While informal statements helped human species to communicate and collaborate, eventually evolving languages themselves, informal statements were not enough for settling arguments, legal disputes or discovering (new) knowledge from existing knowledge. Socrates, Plato’s teacher, is credited with coming up with inductive arguments and universal definitions. Aristotle’s logic consisted of formulating statements and deriving new knowledge (conclusion) from the knowledge contained in a set of statements (&lt;em&gt;premises&lt;/em&gt;) guided by rules that he called &lt;em&gt;syllogisms&lt;/em&gt;. Aristotle, a student of Plato, is credited with the earliest study of logic, &lt;em&gt;formal logic&lt;/em&gt;, in his collected works &lt;a href=&quot;https://archive.org/details/AristotleOrganon/mode/2up&quot;&gt;&lt;em&gt;Organon&lt;/em&gt;&lt;/a&gt;. Plato also contributed to the study of reasoning. While Plato relied on &lt;em&gt;deduction&lt;/em&gt; from &lt;em&gt;a priori&lt;/em&gt; knowledge and postulated that knowledge of things that happen can be deduced from the knowledge of universal ideas, Aristotle’s epistemology postulated that knowledge of things that happen build up to universal knowledge collectively. Together the main recipes for reasoning - induction, deduction and abduction - were discovered that are the pillars for making reasoning infallible.&lt;/p&gt;

&lt;p&gt;Mathematicians developed it further for mathematical reasoning which got shaped, like many mathematical computations, into &lt;em&gt;automated reasoning&lt;/em&gt; using computer programs by computer scientists. Mathematical logic is the cornerstone of systems that are used for reasoning about the correctness of computer programs.&lt;/p&gt;

&lt;p&gt;We can use &lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt;’s Python API to play with it to see what it means to encode English sentences as logical formulas. Once we are able to encode knowledge (in English sentences) we can use well formed rules of logic to deduce new information, also called &lt;em&gt;logical inference&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_parser&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grammar_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'file:///nltk_data/grammars/book_grammars/simple-sem.fcfg'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_parser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grammar_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Angus gives a bone to every dog'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stmt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'SEM'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This gives the semantics in the form of a logical formula:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;all z2.(dog(z2) -&amp;gt; exists z1.(bone(z1) &amp;amp; give(angus,z1,z2)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Further more, one could use the theorem prover that comes along with NLTK to check for consistency among statements and prove goals (conclusion) given a list of statements (premises). For example, one could write&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mini_has_ball&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_expr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'has(mini)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mini2jane&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_expr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pass(mini, jane)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rules&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_expr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'all x. all y. (pass(x, y) -&amp;gt; has(y))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We just encoded some English statements as well as the assumption that if &lt;em&gt;x&lt;/em&gt; passes the ball to &lt;em&gt;y&lt;/em&gt;, then &lt;em&gt;y&lt;/em&gt; will have the ball. We can use a theorem prover to prove a goal that &lt;em&gt;jane has the ball&lt;/em&gt; and check if it follows from our assumptions, i.e. of the goal can be derived given the assumptions&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;jane_has_ball&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_expr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'has(jane)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nltk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Prover9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jane_has_ball&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mini2jane&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and the prover, if it can establish a proof, will return:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This kind of encoding and logical reasoning might be needed to answer questions like the last one we saw in a &lt;a href=&quot;https://dotkay.github.io/2022/03/21/languages-logic-computation&quot;&gt;previous post&lt;/a&gt;. One might also need more expressive logics in order to encode statements about temporal events succinctly. However, languages themselves are stochastic systems which would imply that we would never be able to come up with logically accurate formulations of the sentences we use in everyday conversations, especially when it involves conversations involving languages that are not one’s native language and might have grammatical mistakes and still convey the intended message. Legal documents or financial documents, for example, could be structured in a precise format that might allow one to formulate the sentences in a logical language and make queries that can return an accurate answer that can be reasoned about if needed - think of a healthcare database to which policy makers submit queries in a natural language. A deep learning based approach, which looks at several usage patterns, to arrive at an approximate formulation which would still enable one to deduce new information from a set of facts already provided. This would be more crucial in developing (explainable) AI systems that can be reasoned about, which is especially crucial in healthcare and other critical domains where AI systems are beginning to find their place.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://images.metmuseum.org/CRDImages/ep/original/DP-13139-001.jpg#center&quot; alt=&quot;img&quot; /&gt;
&lt;em&gt;Death of Socrates, by Jacques Louis David (Credits: The Met)&lt;/em&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="NLP" />
      
        <category term="logic" />
      
        <category term="reasoning" />
      
        <category term="computation" />
      
        <category term="NLTK" />
      

      

      
        <summary type="html">Human languages developed and evolved over thousands of years. Though most languages have a structure (defined by the rules of grammar) we think of spoken languages as a series of informal statements (“informal” compared to the “formal” rigor of mathematical statements). While informal statements helped human species to communicate and collaborate, eventually evolving languages themselves, informal statements were not enough for settling arguments, legal disputes or discovering (new) knowledge from existing knowledge. Socrates, Plato’s teacher, is credited with coming up with inductive arguments and universal definitions. Aristotle’s logic consisted of formulating statements and deriving new knowledge (conclusion) from the knowledge contained in a set of statements (premises) guided by rules that he called syllogisms. Aristotle, a student of Plato, is credited with the earliest study of logic, formal logic, in his collected works Organon. Plato also contributed to the study of reasoning. While Plato relied on deduction from a priori knowledge and postulated that knowledge of things that happen can be deduced from the knowledge of universal ideas, Aristotle’s epistemology postulated that knowledge of things that happen build up to universal knowledge collectively. Together the main recipes for reasoning - induction, deduction and abduction - were discovered that are the pillars for making reasoning infallible. Mathematicians developed it further for mathematical reasoning which got shaped, like many mathematical computations, into automated reasoning using computer programs by computer scientists. Mathematical logic is the cornerstone of systems that are used for reasoning about the correctness of computer programs. We can use NLTK’s Python API to play with it to see what it means to encode English sentences as logical formulas. Once we are able to encode knowledge (in English sentences) we can use well formed rules of logic to deduce new information, also called logical inference. import nltk from nltk import load_parser grammar_file = 'file:///nltk_data/grammars/book_grammars/simple-sem.fcfg' lp = load_parser(grammar_file) stmt = 'Angus gives a bone to every dog' trees = lp.parse(stmt.split()) for tree in trees: print(tree.label()['SEM']) This gives the semantics in the form of a logical formula: all z2.(dog(z2) -&amp;gt; exists z1.(bone(z1) &amp;amp; give(angus,z1,z2))) Further more, one could use the theorem prover that comes along with NLTK to check for consistency among statements and prove goals (conclusion) given a list of statements (premises). For example, one could write mini_has_ball = read_expr('has(mini)') mini2jane = read_expr('pass(mini, jane)') rules = read_expr('all x. all y. (pass(x, y) -&amp;gt; has(y))') We just encoded some English statements as well as the assumption that if x passes the ball to y, then y will have the ball. We can use a theorem prover to prove a goal that jane has the ball and check if it follows from our assumptions, i.e. of the goal can be derived given the assumptions jane_has_ball = read_expr('has(jane)') prover = nltk.Prover9() prover.prove(jane_has_ball, [rules, mini2jane]) and the prover, if it can establish a proof, will return: True This kind of encoding and logical reasoning might be needed to answer questions like the last one we saw in a previous post. One might also need more expressive logics in order to encode statements about temporal events succinctly. However, languages themselves are stochastic systems which would imply that we would never be able to come up with logically accurate formulations of the sentences we use in everyday conversations, especially when it involves conversations involving languages that are not one’s native language and might have grammatical mistakes and still convey the intended message. Legal documents or financial documents, for example, could be structured in a precise format that might allow one to formulate the sentences in a logical language and make queries that can return an accurate answer that can be reasoned about if needed - think of a healthcare database to which policy makers submit queries in a natural language. A deep learning based approach, which looks at several usage patterns, to arrive at an approximate formulation which would still enable one to deduce new information from a set of facts already provided. This would be more crucial in developing (explainable) AI systems that can be reasoned about, which is especially crucial in healthcare and other critical domains where AI systems are beginning to find their place. Death of Socrates, by Jacques Louis David (Credits: The Met)</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Languages, Logic and Computation →</title>
      
      <link href="https://dotkay.github.io/2022/03/21/languages-logic-computation/" rel="alternate" type="text/html" title="Languages, Logic and Computation" />
      <published>2022-03-21T00:00:00+00:00</published>
      <updated>2022-03-21T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2022/03/21/languages-logic-computation</id>
      <content type="html" xml:base="https://dotkay.github.io/2022/03/21/languages-logic-computation/">&lt;p&gt;A &lt;a href=&quot;https://dotkay.github.io/2022/02/22/languages-culture-communication&quot;&gt;previous post&lt;/a&gt; spoke about state-of-the-art NLP systems that are based on Transformers and some of the challenges around using such systems for translations. Although they could translate almost accurately, they may not match the nuances of spoken (informal) languages. There are other challenges around &lt;em&gt;Question Answering&lt;/em&gt; systems - another tasks such NLP systems are capable of doing. In this post, this is explored futher to see when some computation (as in logical inference rather than mathematical inference) is needed to come up with an answer. We play with &lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt; APIs to see how the state-of-the-art NLP systems excel at language understanding and also where they have gaps to fill.&lt;/p&gt;

&lt;p&gt;English, like many other languages, is a little ambiguous. There are well established rules of English grammar but I believe there are several statements in common parlance that have introduce some ambiguity. While I tried &lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;, in particular the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt; engine on a variety of English statements and the results were astounding, when I kept going on, there were some statements where the answer was not exactly clear.&lt;/p&gt;

&lt;p&gt;One of the was what &lt;em&gt;she&lt;/em&gt; refers to in a statement like &lt;em&gt;Anna hit Clara because she was drunk&lt;/em&gt;. Though it looks ambiguous I believe as &lt;em&gt;Anna&lt;/em&gt; is the subject, the &lt;em&gt;she&lt;/em&gt; in the statement must refer to &lt;em&gt;Anna&lt;/em&gt;. OpenAI does seem to tell me that the &lt;em&gt;she&lt;/em&gt; in the statement refers to &lt;em&gt;Anna&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create(
      engine=&quot;text-davinci-002&quot;,
      prompt=&quot;Anna hit Clara because she was drunk.\n\nQ: Who was drunk?\nA:&quot;,
      top_p=1,
      ...
    )
&amp;gt;&amp;gt;&amp;gt; print(response)
{
  &quot;choices&quot;: [
    {
      ...
      &quot;text&quot;: &quot; Anna was drunk.&quot;
    }
  ],
...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If I rephrase the above statement as &lt;em&gt;Anna hit Clara because she was drunk. Was Clara drunk?&lt;/em&gt;, OpenAI was humble enough to admit that it does not know, which is impressive as there isn’t enough context to infer if &lt;em&gt;Clara&lt;/em&gt; was drunk.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create(
      engine=&quot;text-davinci-002&quot;,
      prompt=&quot;Anna hit Clara because she was drunk.\n\nQ: Was Clara drunk?\nA:&quot;,
      ...
      stop=[&quot;\n&quot;]
    )
&amp;gt;&amp;gt;&amp;gt; print(response)
{
  &quot;choices&quot;: [
    {
      ...
      &quot;text&quot;: &quot; I don't know.&quot;
    }
  ],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What impressed me the most was the ability to do formulaic calculations and return an answer.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create(
      engine=&quot;text-davinci-002&quot;,
      prompt=&quot;The sale price of a decent house in California is about a million dollars. 20%
      of the price needs to be paid as down payment. In addition, the county levies a 
      property tax of 1.2% on the sale price of the house. John takes a 30 year fixed rate 
      mortgage at 3.5% interest rate.\n\nQ: What would be John's monthly cost of ownership? 
      \nA:&quot;,
      ...
    )
&amp;gt;&amp;gt;&amp;gt; print(response)
{
  &quot;choices&quot;: [
    {
      ... 
      &quot;text&quot;: &quot; John's monthly cost of ownership would be $4,167.&quot;
    }
  ],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While the ability to solve when numerical information was provided was too impressive, there was some struggle when the answer involved logical interpretation and numerical computation. For example, the query in the following requires interpreting what “boundary” is and how it contributes to the answer.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create(
      engine=&quot;text-davinci-002&quot;,
      prompt=&quot;Dhoni scored a single off the first ball of the 11th over. Sachin scored two
      consecutive boundaries followed by a single. Dhoni hit the biggest six of the match. 
      He then followed it up with a single and retained strike. \n\nQ: How many runs were 
      scored off the 11th over? \nA:&quot;,
      ...
    )
&amp;gt;&amp;gt;&amp;gt; print(response)
{
  &quot;choices&quot;: [
    {
      ...
      &quot;text&quot;: &quot; 15&quot;
    }
  ],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Human languages are such that they involve logical interpretation, deduction and computation. It is precisely this challenge that leads to misinterpretation, miscommunication and chaos - you can think of the last time a celebrity or a politican claimed his comments were misinterpreted or taken out of context. However, we as human beings deal with these every day and are attuned to the fallacies and inaccuracies. Our communication system, the one that distinguishes us from many other species, is itself a stochastic one.&lt;/p&gt;

&lt;p&gt;Some statements require logical deduction which is challenging for deep learning systems (that learn from patterns contained in a training corpus). For example,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create(
      engine=&quot;text-davinci-002&quot;,
      prompt=&quot;marco passed the ball to miel, miel quickly gave it to don who passed it on to
      sky. sky ran with it for a while and gave it to anna.\n\nQ: Does marco have the ball 
      now?\nA:&quot;,
      ...
   )
   &amp;gt;&amp;gt;&amp;gt; print(response)
{
  &quot;choices&quot;: [
    {
      ...
      &quot;text&quot;: &quot; No&quot;
    }
  ],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While the system is able to deduce that &lt;em&gt;marco&lt;/em&gt; does not have the ball, a deduction performed to answer &lt;em&gt;who has the ball?&lt;/em&gt; doesn’t seem to work well.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create(
      engine=&quot;text-davinci-002&quot;,
      prompt=&quot;marco passed the ball to miel, miel quickly gave it to don who passed it on to
      sky. sky ran with it for a while and gave it to anna.\n\nQ: Who has the ball now?\nA:&quot;,
      ...
    )
&amp;gt;&amp;gt;&amp;gt; print(response)
{
  &quot;choices&quot;: [
    {
      ...
      &quot;text&quot;: &quot;&quot;
    }
  ],
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is immense research interest in this area and the progress being made is as impressive as that has been achieved so far. Hopefully, in a future post, I will be able to show an NLP system that combines logical reasoning with deep learning and is able to answer much more fancier questions. I am looking forward to it.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="NLP" />
      
        <category term="logic" />
      
        <category term="computation" />
      
        <category term="OpenAI" />
      

      

      
        <summary type="html">A previous post spoke about state-of-the-art NLP systems that are based on Transformers and some of the challenges around using such systems for translations. Although they could translate almost accurately, they may not match the nuances of spoken (informal) languages. There are other challenges around Question Answering systems - another tasks such NLP systems are capable of doing. In this post, this is explored futher to see when some computation (as in logical inference rather than mathematical inference) is needed to come up with an answer. We play with OpenAI APIs to see how the state-of-the-art NLP systems excel at language understanding and also where they have gaps to fill. English, like many other languages, is a little ambiguous. There are well established rules of English grammar but I believe there are several statements in common parlance that have introduce some ambiguity. While I tried OpenAI, in particular the text-davinci-002 engine on a variety of English statements and the results were astounding, when I kept going on, there were some statements where the answer was not exactly clear. One of the was what she refers to in a statement like Anna hit Clara because she was drunk. Though it looks ambiguous I believe as Anna is the subject, the she in the statement must refer to Anna. OpenAI does seem to tell me that the she in the statement refers to Anna. &amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create( engine=&quot;text-davinci-002&quot;, prompt=&quot;Anna hit Clara because she was drunk.\n\nQ: Who was drunk?\nA:&quot;, top_p=1, ... ) &amp;gt;&amp;gt;&amp;gt; print(response) { &quot;choices&quot;: [ { ... &quot;text&quot;: &quot; Anna was drunk.&quot; } ], ... } If I rephrase the above statement as Anna hit Clara because she was drunk. Was Clara drunk?, OpenAI was humble enough to admit that it does not know, which is impressive as there isn’t enough context to infer if Clara was drunk. &amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create( engine=&quot;text-davinci-002&quot;, prompt=&quot;Anna hit Clara because she was drunk.\n\nQ: Was Clara drunk?\nA:&quot;, ... stop=[&quot;\n&quot;] ) &amp;gt;&amp;gt;&amp;gt; print(response) { &quot;choices&quot;: [ { ... &quot;text&quot;: &quot; I don't know.&quot; } ], ... } What impressed me the most was the ability to do formulaic calculations and return an answer. &amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create( engine=&quot;text-davinci-002&quot;, prompt=&quot;The sale price of a decent house in California is about a million dollars. 20% of the price needs to be paid as down payment. In addition, the county levies a property tax of 1.2% on the sale price of the house. John takes a 30 year fixed rate mortgage at 3.5% interest rate.\n\nQ: What would be John's monthly cost of ownership? \nA:&quot;, ... ) &amp;gt;&amp;gt;&amp;gt; print(response) { &quot;choices&quot;: [ { ... &quot;text&quot;: &quot; John's monthly cost of ownership would be $4,167.&quot; } ], ... } While the ability to solve when numerical information was provided was too impressive, there was some struggle when the answer involved logical interpretation and numerical computation. For example, the query in the following requires interpreting what “boundary” is and how it contributes to the answer. &amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create( engine=&quot;text-davinci-002&quot;, prompt=&quot;Dhoni scored a single off the first ball of the 11th over. Sachin scored two consecutive boundaries followed by a single. Dhoni hit the biggest six of the match. He then followed it up with a single and retained strike. \n\nQ: How many runs were scored off the 11th over? \nA:&quot;, ... ) &amp;gt;&amp;gt;&amp;gt; print(response) { &quot;choices&quot;: [ { ... &quot;text&quot;: &quot; 15&quot; } ], ... } Human languages are such that they involve logical interpretation, deduction and computation. It is precisely this challenge that leads to misinterpretation, miscommunication and chaos - you can think of the last time a celebrity or a politican claimed his comments were misinterpreted or taken out of context. However, we as human beings deal with these every day and are attuned to the fallacies and inaccuracies. Our communication system, the one that distinguishes us from many other species, is itself a stochastic one. Some statements require logical deduction which is challenging for deep learning systems (that learn from patterns contained in a training corpus). For example, &amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create( engine=&quot;text-davinci-002&quot;, prompt=&quot;marco passed the ball to miel, miel quickly gave it to don who passed it on to sky. sky ran with it for a while and gave it to anna.\n\nQ: Does marco have the ball now?\nA:&quot;, ... ) &amp;gt;&amp;gt;&amp;gt; print(response) { &quot;choices&quot;: [ { ... &quot;text&quot;: &quot; No&quot; } ], ... } While the system is able to deduce that marco does not have the ball, a deduction performed to answer who has the ball? doesn’t seem to work well. &amp;gt;&amp;gt;&amp;gt; response = openai.Completion.create( engine=&quot;text-davinci-002&quot;, prompt=&quot;marco passed the ball to miel, miel quickly gave it to don who passed it on to sky. sky ran with it for a while and gave it to anna.\n\nQ: Who has the ball now?\nA:&quot;, ... ) &amp;gt;&amp;gt;&amp;gt; print(response) { &quot;choices&quot;: [ { ... &quot;text&quot;: &quot;&quot; } ], ... } There is immense research interest in this area and the progress being made is as impressive as that has been achieved so far. Hopefully, in a future post, I will be able to show an NLP system that combines logical reasoning with deep learning and is able to answer much more fancier questions. I am looking forward to it.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Languages, Culture and Communication →</title>
      
      <link href="https://dotkay.github.io/2022/02/22/languages-culture-communication/" rel="alternate" type="text/html" title="Languages, Culture and Communication" />
      <published>2022-02-22T00:00:00+00:00</published>
      <updated>2022-02-22T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2022/02/22/languages-culture-communication</id>
      <content type="html" xml:base="https://dotkay.github.io/2022/02/22/languages-culture-communication/">&lt;p&gt;Languages have always played a very important role in society and in our cultural evolutions. It has been the vehicle that allowed human beings to collaborate, coexist and achieve great things as well as to deceive, conspire and destroy. Languages have also evolved over time carrying with them many cultural norms, shades of history while also losing some. The same language might sound differently across an ocean or even across geographic land boundaries. Some languages like Sanskrit, though less ambiguous and more efficient, are struggling to survive while others like English are flourishing despite being more ambiguous. In the age of AI based natural language processing (NLP) is it possible to encode some of these norms and cultural connotations in order to make automated translations more close to native spoken languages?&lt;/p&gt;

&lt;p&gt;NLP systems are a commonplace today, mostly in the form of personal assistants like Alexa and chatbots like a customer service system. While they are able to understand and respond to usual queries or translate text to a surprising level of accuracy, they are far from matching human levels of communication. My claim is that NLP research is yet to discover the means to encode the several cultural connotations that come with languages, especially while translating from one to another. While NLP algorithms like LSTM and &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;&gt;Transformers&lt;/a&gt; implicitly encode some of the context needed for better understanding of a statement, a pair of languages might be too different to capture all the context implicitly. Is capturing all of the context necessary? May not be. However, capturing important nuances might be.&lt;/p&gt;

&lt;p&gt;I have been playing around with the &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; API for quite sometime and really enjoying the easeness of use - ability to work with different popular transformer models for different types of NLP tasks. And of course, the ability to use several different famous models makes the result almost unbelievable. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline()&lt;/code&gt; - fundamental component in the &lt;em&gt;Transformers&lt;/em&gt; library serves as a pipeline between the different steps in an NLP pipeline. For example, we may want to connect a preprocessing step to an analysis step and then to a post-processing step. I believe the ease of use and the availability of several models makes Hugging Face the go-to tool for NLP.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When instantiating, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline()&lt;/code&gt; expects the user to provide the NLP task we are interested in performing as well as an optional model (when not provided, a default model is used).&lt;/p&gt;

&lt;p&gt;We can create, for example, a sentiment analyzer by providing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sentiment-analysis&lt;/code&gt; as the task. If we do not specify a model to be used, it defaults to Distilbert model.&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentiment-analysis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can pass strings that we want to analyze the sentiment for.&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I plan to learn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and it returns the sentiment and a score of the sentiment.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; [{'label': 'POSITIVE', 'score': 0.9988747239112854}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So “learn” seems to be a positive thing to do. Let’s keep going with this:&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I plan to learn Spanish&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'POSITIVE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9941837191581726&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, learning Spanish also seems to be a positive thing to do. However,&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I plan to learn some more Spanish.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'NEGATIVE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9369450807571411&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I am not sure why, but learning some more seems to have a negative connotation (may be “some more” indicates some lacking that one is trying to improve upon?). However, if I add an additional phrase the overall sentiment seems to become positive.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I plan to learn some more Spanish. I want to be able to live and work in Spain.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'POSITIVE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9970537424087524&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These are just some of the challenges with current NLP systems. The result could be interpreted in several ways and if the training (of which a model is a result of) was done on specific set of (data from a specific culture or a communication pattern), some phrases might get inferred as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEGATIVE&lt;/code&gt; though it would not seem negative to the user of such systems. Another example showing a subtle difference:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I plan to learn enough Corporate Finance in order to get a great job&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'NEGATIVE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5767139792442322&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;returns a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEGATIVE&lt;/code&gt; sentiment while&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sanalyzer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I plan to learn Corporate Finance in order to get a great job&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'POSITIVE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.902138888835907&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;returns a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POSITIVE&lt;/code&gt; one:&lt;/p&gt;

&lt;p&gt;More intriguing are the expressions that we use depending on cultures and traditions. In many languages, we use different pronouns and verb conjugations depending on if we are talking to elders using a formal language or talking to friends or children (informal language) - like &lt;em&gt;vous&lt;/em&gt; vs &lt;em&gt;tu&lt;/em&gt; in French, &lt;em&gt;aap&lt;/em&gt; vs &lt;em&gt;tum&lt;/em&gt; in Hindi, &lt;em&gt;avar&lt;/em&gt; vs &lt;em&gt;avan&lt;/em&gt; in Tamil, &lt;em&gt;iyaal&lt;/em&gt; vs &lt;em&gt;ival&lt;/em&gt; in Malayalam, etc. Humans have learnt to make that judgement call even when we talk over the phone - we choose formal or informal language depending on how much respect or importance we attach to the person we are talking to, even if it is a stranger and we are speaking to them for the first time. However, NLP systems do not have that context explicitly encoded and it is difficult to implicitly encode them as some languages do not have such differences and while translating from such languages, one would have to possibly over-correct.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;en_fr_translator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;translation_en_to_fr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;en_fr_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Would you like some coffee?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Avez-vous envie de prendre du café ?'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we see, we could offer coffee to a close friend (informal language) or we could be a barista offering coffee to a stranger or a customer (using formal language). These are very difficult to encode implicitly. Even when there are some clues in the statement, like the following sentence which talks about &lt;em&gt;balloons&lt;/em&gt; which most likely only children would be interested in, most systems would prefer a formal language in order to be absolutely correct.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;en_fr_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Do you want a balloon?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Voulez-vous un ballon?'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are also everyday language (colloquial) that differs from a more grammatically correct business or literary language. At times, the differences between the two is so large that chatbots may sound weird. Let us contrast English-French translation with French-English translation to check this.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;en_fr_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;There's no one here.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Il n'y a personne ici.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In spoken French, &lt;a href=&quot;https://qr.ae/pvKYf3&quot;&gt;one usually leaves out the negation (&lt;em&gt;ne&lt;/em&gt;) in certain cases&lt;/a&gt; and so “&lt;em&gt;Il y a personne ici.&lt;/em&gt;” would also actually mean “&lt;em&gt;There’s no one here.&lt;/em&gt;”&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fr_en_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Il y a personne ici.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;There's no one here.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another example, which might sound very counter-intuitive for people new to the French language.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;en_fr_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I have&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;J'ai&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;en_fr_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;more bread.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'plus de pain.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Such statements are not always compositional.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fr_en_translator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;J'ai plus de pain.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'translation_text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;I don't have any more bread.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Many NLP systems are trained on formal and informal phrases and hence would be great translators, they may not always be a great teacher. The fluidity of languages makes languages evolve, creating new words, phrases, letting go some rules of grammar and these are exactly what make languages beatiful and a treasure trove for linguists. These are also the same challenges that make NLP a fascinating subject for Computer Scientists. I am eagerly looking forward to understanding the research in this area hoping that there would be some architecture that would enable us encode some of these connotations implicitly, making NLP systems crawl a little closer towards human-like spoken languages.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="NLP" />
      
        <category term="Languages" />
      
        <category term="Hugging Face" />
      

      

      
        <summary type="html">Languages have always played a very important role in society and in our cultural evolutions. It has been the vehicle that allowed human beings to collaborate, coexist and achieve great things as well as to deceive, conspire and destroy. Languages have also evolved over time carrying with them many cultural norms, shades of history while also losing some. The same language might sound differently across an ocean or even across geographic land boundaries. Some languages like Sanskrit, though less ambiguous and more efficient, are struggling to survive while others like English are flourishing despite being more ambiguous. In the age of AI based natural language processing (NLP) is it possible to encode some of these norms and cultural connotations in order to make automated translations more close to native spoken languages? NLP systems are a commonplace today, mostly in the form of personal assistants like Alexa and chatbots like a customer service system. While they are able to understand and respond to usual queries or translate text to a surprising level of accuracy, they are far from matching human levels of communication. My claim is that NLP research is yet to discover the means to encode the several cultural connotations that come with languages, especially while translating from one to another. While NLP algorithms like LSTM and Transformers implicitly encode some of the context needed for better understanding of a statement, a pair of languages might be too different to capture all the context implicitly. Is capturing all of the context necessary? May not be. However, capturing important nuances might be. I have been playing around with the Hugging Face API for quite sometime and really enjoying the easeness of use - ability to work with different popular transformer models for different types of NLP tasks. And of course, the ability to use several different famous models makes the result almost unbelievable. The pipeline() - fundamental component in the Transformers library serves as a pipeline between the different steps in an NLP pipeline. For example, we may want to connect a preprocessing step to an analysis step and then to a post-processing step. I believe the ease of use and the availability of several models makes Hugging Face the go-to tool for NLP. from transformers import pipeline When instantiating, the pipeline() expects the user to provide the NLP task we are interested in performing as well as an optional model (when not provided, a default model is used). We can create, for example, a sentiment analyzer by providing sentiment-analysis as the task. If we do not specify a model to be used, it defaults to Distilbert model. sanalyzer = pipeline('sentiment-analysis') We can pass strings that we want to analyze the sentiment for. sanalyzer(&quot;I plan to learn&quot;) and it returns the sentiment and a score of the sentiment. &amp;gt; [{'label': 'POSITIVE', 'score': 0.9988747239112854}] So “learn” seems to be a positive thing to do. Let’s keep going with this: sanalyzer(&quot;I plan to learn Spanish&quot;) &amp;gt; [{'label': 'POSITIVE', 'score': 0.9941837191581726}] So, learning Spanish also seems to be a positive thing to do. However, sanalyzer(&quot;I plan to learn some more Spanish.&quot;) &amp;gt; [{'label': 'NEGATIVE', 'score': 0.9369450807571411}] I am not sure why, but learning some more seems to have a negative connotation (may be “some more” indicates some lacking that one is trying to improve upon?). However, if I add an additional phrase the overall sentiment seems to become positive. sanalyzer(&quot;I plan to learn some more Spanish. I want to be able to live and work in Spain.&quot;) &amp;gt; [{'label': 'POSITIVE', 'score': 0.9970537424087524}] These are just some of the challenges with current NLP systems. The result could be interpreted in several ways and if the training (of which a model is a result of) was done on specific set of (data from a specific culture or a communication pattern), some phrases might get inferred as NEGATIVE though it would not seem negative to the user of such systems. Another example showing a subtle difference: sanalyzer(&quot;I plan to learn enough Corporate Finance in order to get a great job&quot;) &amp;gt; [{'label': 'NEGATIVE', 'score': 0.5767139792442322}] returns a NEGATIVE sentiment while sanalyzer(&quot;I plan to learn Corporate Finance in order to get a great job&quot;) &amp;gt; [{'label': 'POSITIVE', 'score': 0.902138888835907}] returns a POSITIVE one: More intriguing are the expressions that we use depending on cultures and traditions. In many languages, we use different pronouns and verb conjugations depending on if we are talking to elders using a formal language or talking to friends or children (informal language) - like vous vs tu in French, aap vs tum in Hindi, avar vs avan in Tamil, iyaal vs ival in Malayalam, etc. Humans have learnt to make that judgement call even when we talk over the phone - we choose formal or informal language depending on how much respect or importance we attach to the person we are talking to, even if it is a stranger and we are speaking to them for the first time. However, NLP systems do not have that context explicitly encoded and it is difficult to implicitly encode them as some languages do not have such differences and while translating from such languages, one would have to possibly over-correct. en_fr_translator = pipeline(&quot;translation_en_to_fr&quot;) en_fr_translator(&quot;Would you like some coffee?&quot;) &amp;gt; [{'translation_text': 'Avez-vous envie de prendre du café ?'}] As we see, we could offer coffee to a close friend (informal language) or we could be a barista offering coffee to a stranger or a customer (using formal language). These are very difficult to encode implicitly. Even when there are some clues in the statement, like the following sentence which talks about balloons which most likely only children would be interested in, most systems would prefer a formal language in order to be absolutely correct. en_fr_translator(&quot;Do you want a balloon?&quot;) &amp;gt; [{'translation_text': 'Voulez-vous un ballon?'}] There are also everyday language (colloquial) that differs from a more grammatically correct business or literary language. At times, the differences between the two is so large that chatbots may sound weird. Let us contrast English-French translation with French-English translation to check this. en_fr_translator(&quot;There's no one here.&quot;) &amp;gt; [{'translation_text': &quot;Il n'y a personne ici.&quot;}] In spoken French, one usually leaves out the negation (ne) in certain cases and so “Il y a personne ici.” would also actually mean “There’s no one here.” fr_en_translator(&quot;Il y a personne ici.&quot;) &amp;gt; [{'translation_text': &quot;There's no one here.&quot;}] Another example, which might sound very counter-intuitive for people new to the French language. en_fr_translator(&quot;I have&quot;) &amp;gt; [{'translation_text': &quot;J'ai&quot;}] en_fr_translator(&quot;more bread.&quot;) &amp;gt; [{'translation_text': 'plus de pain.'}] Such statements are not always compositional. fr_en_translator(&quot;J'ai plus de pain.&quot;) &amp;gt; [{'translation_text': &quot;I don't have any more bread.&quot;}] Many NLP systems are trained on formal and informal phrases and hence would be great translators, they may not always be a great teacher. The fluidity of languages makes languages evolve, creating new words, phrases, letting go some rules of grammar and these are exactly what make languages beatiful and a treasure trove for linguists. These are also the same challenges that make NLP a fascinating subject for Computer Scientists. I am eagerly looking forward to understanding the research in this area hoping that there would be some architecture that would enable us encode some of these connotations implicitly, making NLP systems crawl a little closer towards human-like spoken languages.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">The Year in Formula One →</title>
      
      <link href="https://dotkay.github.io/2021/12/21/year-in-formula-one/" rel="alternate" type="text/html" title="The Year in Formula One" />
      <published>2021-12-21T00:00:00+00:00</published>
      <updated>2021-12-21T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/12/21/year-in-formula-one</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/12/21/year-in-formula-one/">&lt;p&gt;If you are someone who has been watching Formula One over the last decade, it would be safe to bet that you agree 2021 has been one of the exciting year in the sport. Lots of uncertainity, lots of controversies, new, young and energetic racers, and a super-busy calendar with 22 races. In this post, we will look at the last decade in the sport while doing so using Python and Pandas to analyze, plot and visualize the data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;constr_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'driver_standings_2010_2021.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;driver_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'f1_2010_2021.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we look at the total points by different teams:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;team_totals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'team'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                           &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                           &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                           &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'team'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;team_totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/f1_constr_2010_2021.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Mercedes is clearly the front runner, most of it contributed by Lewis Hamilton as we will see very soon. Having lived in Italy and a Ferrari Fan, it is sad to see Ferrari in second place and even more sad to see the downward trend for the prancing horse.&lt;/p&gt;

&lt;p&gt;Let’s turn to the drivers who play a key role in making the sport what it is. After Mike Schumacher’s retirement, there was some competition between Sebastian Vettel and Lewis Hamilton and to some extent Fernando Alonso and Nico Rosberg. Unfortunately Sebastian Vettel’s dominance faded away and Lewis Hamilton started dominating every race. Very soon there was a new kid in the block - &lt;em&gt;Max Verstappen&lt;/em&gt;. Young, energetic and impatient, Max Verstappen, racing for RedBull, quickly started dominating the sport throwing a big challenge to Lewis Hamilton. Many predicted his dominance given the last couple of years and if there was even the slightest of doubts, the 2021 racing season would have cleared those away. Just before the last race, Lewis Hamilton and Max Verstappen were tied on the same number of points which shows the sheer scale of the competition.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;driver_totals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'full_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                             &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                             &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                             &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'full_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver_totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/f1_drivers_2010_2021.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can also see the maximum points scored by the drivers in any year. This shows Lewis Hamilton dominating the races that were once dominated by Sebastian Vettel.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;max_pt_drivers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                              &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;driver_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_pt_drivers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'crest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;patches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_pt_drivers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;xy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;xytext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;textcoords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'offset points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;ha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'center'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;va&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'center'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_xticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/f1_driver_max_pts_2010_2021.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can also see which countries have scored the most points. Traditionally, Formula One has been more popular in Europe than the rest of the world. There are and were South American champions but not many of them. I believe this might be due to the cost involved in the sport and few can afford it. Further most of the best automobile makers are from Europe (and few from Japan as well). We can attempt to draw a tree map using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;squarify&lt;/code&gt; in Python.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;squarify&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;country_wins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nationality'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;squarify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country_wins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'points'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country_wins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nationality'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'off'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/f1_country_wins_tree.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;If the tree map makes you think GER (Germany) and GBR (Great Britain) are equal, here is a bar chart showing the actual points.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/f1_country_wins_bar.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In all, 2021 was a fantastic year for Formula One. I wish the most crucial and deciding race had not ended in a controversial way. Sergio Perez played the best team-mate and tons of thanks to Lewis and Max for the entertainment.&lt;/p&gt;

&lt;p&gt;Looking forward to more competition in 2022.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 70%&quot;&gt;
&lt;i&gt;The data is from a Kaggle dataset revised with the last race results of 2021, and the plots were done using matplotlib, seaborn.&lt;/i&gt;
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="formula one" />
      
        <category term="data science" />
      
        <category term="python" />
      

      

      
        <summary type="html">If you are someone who has been watching Formula One over the last decade, it would be safe to bet that you agree 2021 has been one of the exciting year in the sport. Lots of uncertainity, lots of controversies, new, young and energetic racers, and a super-busy calendar with 22 races. In this post, we will look at the last decade in the sport while doing so using Python and Pandas to analyze, plot and visualize the data. import pandas as pd constr_df = pd.read_csv('driver_standings_2010_2021.csv') driver_df = pd.read_csv('f1_2010_2021.csv') If we look at the total points by different teams: team_totals = pd.DataFrame(constr_df.groupby(['team'])['points'] .sum() .sort_values(ascending=False)) .reset_index() sns.barplot(x='team', y='points', data=team_totals) ... Mercedes is clearly the front runner, most of it contributed by Lewis Hamilton as we will see very soon. Having lived in Italy and a Ferrari Fan, it is sad to see Ferrari in second place and even more sad to see the downward trend for the prancing horse. Let’s turn to the drivers who play a key role in making the sport what it is. After Mike Schumacher’s retirement, there was some competition between Sebastian Vettel and Lewis Hamilton and to some extent Fernando Alonso and Nico Rosberg. Unfortunately Sebastian Vettel’s dominance faded away and Lewis Hamilton started dominating every race. Very soon there was a new kid in the block - Max Verstappen. Young, energetic and impatient, Max Verstappen, racing for RedBull, quickly started dominating the sport throwing a big challenge to Lewis Hamilton. Many predicted his dominance given the last couple of years and if there was even the slightest of doubts, the 2021 racing season would have cleared those away. Just before the last race, Lewis Hamilton and Max Verstappen were tied on the same number of points which shows the sheer scale of the competition. driver_totals = pd.DataFrame(driver_df.groupby(['full_name'])['points'] .sum() .sort_values(ascending=False)) .reset_index() sns.barplot(x='full_name', y='points', data=driver_totals) ... We can also see the maximum points scored by the drivers in any year. This shows Lewis Hamilton dominating the races that were once dominated by Sebastian Vettel. max_pt_drivers = pd.DataFrame(driver_df[driver_df.groupby(['year'])['points'] .transform('max') == driver_df['points']]) ax = sns.barplot(x='year', y='points', data=max_pt_drivers, palette='crest') for p, (i, txt) in zip(ax.patches, enumerate(max_pt_drivers['name'])): ax.annotate(txt, xy=(p.get_x() + p.get_width() / 2, p.get_height()), xytext=(0, 8), textcoords='offset points', ha='center', va='center', size=12 ) ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=14); ... We can also see which countries have scored the most points. Traditionally, Formula One has been more popular in Europe than the rest of the world. There are and were South American champions but not many of them. I believe this might be due to the cost involved in the sport and few can afford it. Further most of the best automobile makers are from Europe (and few from Japan as well). We can attempt to draw a tree map using squarify in Python. import squarify country_wins = pd.DataFrame(driver_df.groupby(['nationality'])['points'] .sum() .sort_values(ascending=False)) .reset_index() squarify.plot(sizes=country_wins['points'][:21], label=country_wins['nationality'][:21], alpha=0.8) plt.axis('off') If the tree map makes you think GER (Germany) and GBR (Great Britain) are equal, here is a bar chart showing the actual points. In all, 2021 was a fantastic year for Formula One. I wish the most crucial and deciding race had not ended in a controversial way. Sergio Perez played the best team-mate and tons of thanks to Lewis and Max for the entertainment. Looking forward to more competition in 2022. The data is from a Kaggle dataset revised with the last race results of 2021, and the plots were done using matplotlib, seaborn.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Opportunities in Metaverse →</title>
      
      <link href="https://dotkay.github.io/2021/11/21/metaverse-opportunities/" rel="alternate" type="text/html" title="Opportunities in Metaverse" />
      <published>2021-11-21T00:00:00+00:00</published>
      <updated>2021-11-21T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/11/21/metaverse-opportunities</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/11/21/metaverse-opportunities/">&lt;p&gt;A &lt;a href=&quot;https://dotkay.github.io/2021/10/01/metaverse-drivers/&quot;&gt;previous post&lt;/a&gt; outlined several drivers of metaverse. While it is still in its nascent stages, metaverse would bring truly immersive environments whose applications would span beyond video games. With the trends in virtual and augmented reality, there are immense economic opportunities as well as implications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gaming:&lt;/strong&gt; The most widely accepted and widely talked about application is, of course, &lt;em&gt;gaming&lt;/em&gt;. Game developers have been creating virtual worlds for decades - so what is different now?. With advances in hardware accleration and Deep Learning algorithms, virtual world experience is becoming more and more realistic than in previous generations of games. For example, techniques like real-time ray tracing were known for a very long time. These are beginning to become practical and a commonplace only recently. So are techniques like super sampling techniques using Deep Learning, as well as other compression techniques that enable creating Bounding Volume Hierarchies (BVH) packing more details about the scene. With Deep Learning techniques playing an increasing important role in simulating physics, more interactive and immersive games can be developed that generate close to accurate physics (flames, movement, flow, etc.) making graphics more realistic. There is enough economic opportunity in gaming and with game streaming it has been growing at an accelerated pace. One can check out the &lt;a href=&quot;https://twitchtracker.com/statistics&quot;&gt;Twitch Streaming Statistics&lt;/a&gt; and get convinced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advertising:&lt;/strong&gt; If I am watching a live sports event on TV (like soccer) from San Francisco, I would see advertisements pertaining to the local (US) market, while the same game, in an Italian feed, will have advertisements tailored to the European market. This is already a well-known and mature technology called &lt;em&gt;virtual replacement perimeter technology&lt;/em&gt;. &lt;a href=&quot;https://supponor.com/&quot;&gt;&lt;em&gt;Supponor&lt;/em&gt;&lt;/a&gt; is one of the leaders in this space. Metaverse would enable to take this a step further. The commercials would not only get specialized or tailored according to the local market where it is being broadcasted live but also according to each viewer’s personal preferences. Much like how Google’s digital advertising and the growth of mobile and social networks allowed personalized, targeted ads, metaverse would offer such opportunities in a more immersive arena. Technologies pioneered by companies like &lt;a href=&quot;https://www.youtube.com/watch?v=rLnZaItawYg&quot;&gt;Mirriad&lt;/a&gt; for product placement would be extremely valuable and amenable to personalization, not just for live concerts and live sports but many varieties of content created for the metaverse. Of course, there needs to be sufficient guard-rails and transparency and there would also be renewed &lt;a href=&quot;https://www.youtube.com/watch?v=E_F5GxCwizc&quot;&gt;debates on native advertising&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Factory Automation:&lt;/strong&gt; Like gaming, this is one of the other avenues being talked about. There is one major difference between the virtual world (metaverse) created for gaming and the one created for factory automation purposes. For gaming, the artifacts that make up the metaverse is completely upto the imagination of the creator. However, in the case of factory automation, the digital twin has to replicate a physical structure that already exists. Any change in structure or function created (imagine the direction of conveyor belt in an assembly line) in the physical world has to be reflected in the digital twin. There is also less freedom on what artifacts could to be in the digital twin (metaverse). If many factories have identical assembly lines (like many traditional automobile factories or soft-drink bottling units, etc.) one could work with a templated setup. This would involve creating the artifacts common to all these templates (using Graphics SW tools, videos of factory, AI, etc,.). Further even if many factories share common templated factory floors at a high-level, there might be efficiencies that are introduced at the micro level (kinds of robot-arms they use, kinds of automated inspection mechanisms, etc,.) and these would be the key differentiators leading to competitive advantage. There might be commodity goods made by manufacturers who have no apparent competitive advantage over other similar manufacturers (think of zippers, nails, bottle-caps, automotive brakes and lately some semiconductor components, etc,.) for whom creating a digital twin would be easier. But they are also the ones who would move slowly on such innovations as their business becomes more and more commoditized. On the other hand factories who have a great competitive advantage (in terms of operational efficiencies) can leapfrog by having a centralized view of global factories and bringing them all to the same efficient mode.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E-commerce:&lt;/strong&gt; Advances in technologies like photogrammetry and depth-sensing cameras becoming a commonplace in most smartphones can bring about new innovations in e-commerce. Most of e-commerce today is centered around purchase based on reviews from trying the product offline. It becomes interesting when purchases of clothes and accessories are made as one has to trust the size, fit, etc,. Customers tend to purchase multiple sizes only to return some of them back. Though many never realize, the returns in brick-and-mortar stores is around 8-9% while online retailers can see &lt;a href=&quot;https://www.theatlantic.com/magazine/archive/2021/11/free-returns-online-shopping/620169/&quot;&gt;as much as 30% of the products returned&lt;/a&gt;. It runs to &lt;a href=&quot;https://nrf.com/media-center/press-releases/428-billion-merchandise-returned-2020&quot;&gt;billions of dollars worth&lt;/a&gt; of merchandise. &lt;em&gt;Virtual trial rooms&lt;/em&gt; could pop up at many points of interests (shopping malls, cinemas, etc.) or even in your living room (using a VR head-set and a phone camera or a Kinect system) that would allow one to try out new clothes. It could be tied to other marketing avenues (cross-selling) like selecting a design from a fashion show or a TV series and trying them as and when watching those shows.&lt;/p&gt;

&lt;p&gt;High-end fashion is already experimenting with &lt;a href=&quot;https://www.voguebusiness.com/technology/the-augmented-and-virtual-reality-opportunity-for-luxury&quot;&gt;augmented reality&lt;/a&gt;. Who doesn’t want to go watch virtual French Open sitting next to friends from around the world while wearing your favorite Rafa Nike virutal t-shirt having the &lt;em&gt;Raging Bull&lt;/em&gt; logo? Merchandises are already common in many video games and it could seamlessly get into fashion and other virutal events. Given the growth in influencer economy, it would also be an innovative way for amateur fashion designers to showcase their designs and their brand.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Healthcare and Bio-informatics:&lt;/strong&gt;
Over the last decade, we have seen increased adoption of robotic surgery - devices like &lt;a href=&quot;https://www.intuitive.com/en-us/products-and-services/da-vinci&quot;&gt;DaVinci systems&lt;/a&gt; have been installed in several hospitals around the globe as it minimizes direct human contact during surgery and reduces infection rates. However, Da Vinci is a robotic arm controlled by a skilled surgeon along with mounted cameras and associated image processing software. With decreasing cost of sensors and advancements in software, a sophisticated array of sensors attached to the robotic arm could provide vital information overlaid (augmented reality) on the camera images of the surgery would provide real-time information that could aid in real-time and critical decision making.  With advancements in 5G and networking bandwidths, Da Vinci like systems could one day make remote surgeries possible. Here’s where metaverse systems could play a huge role. Effectively collaborating with different expert surgeons connected to the robotic arm from different parts of the globe would require the image processing software to be capable of showing different views, different sets of vital information in real-time to different surgical experts and also keep each of the surgeons informed of what the other surgeon is trying to do. This requires a lot of image processing, processing data from different sensors in real-time, syncronizing the views of different surgeons and doing it all in real-time. Technologies such as these would also play a vital role in medical education, to train more professionals in places where such high-end technologies may not be very common.&lt;/p&gt;

&lt;p&gt;MRI is another evolving technology with more and more software components, especially image fusion and augmented reality components, along with advances in physics. There is a &lt;a href=&quot;https://global.canon/en/technology/support28.html&quot;&gt;very nice explanation of MRI in Canon’s webpage&lt;/a&gt;. A keen observer would be able to already notice the various places where imaging technology could be improved many folds using techniques like &lt;a href=&quot;https://www.nvidia.com/en-us/geforce/technologies/dlss/&quot;&gt;DLSS&lt;/a&gt; and the like. Image processing applications in a plethora of fields are getting better using Deep Learning techniques, including such &lt;a href=&quot;https://us.medical.canon/products/magnetic-resonance/aice/&quot;&gt;medical imaging systems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Research in &lt;em&gt;genome sequencing&lt;/em&gt; has advanced leaps and bounds over the last decade. Whole genome sequencing (WGS) analyses developed using systems like Illumina provide a high-resolution base-by-base view of the genome. Softwares like TruSight combine the features used by different experts all in one dashboard and provide several ways of visualization and analysis. Softwares like these are testament to the convergence of hardware, system software, graphics and image processing and collaboration softwares as well as techniques like machine learning, all of which are required for research in areas like bio-informatics. With metaverse like platforms many experts can tap into the same analysis done in one molecular biology lab, and can sort them in different ways depending on the remote biologist’s expertise. While one might be an expert in variant filtering, the other might be an expert in evidence retrieval.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fitness and Active Sports:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Though technologies in fitness got a big boost during the pandemic, mostly due to the interest in exercise equipments like Peloton bikes, sports science has been an active field of research by many global brands like &lt;a href=&quot;https://www.youtube.com/watch?v=paY4xxiQNEU&quot;&gt;Nike&lt;/a&gt;. Companies like Google have already mapped the many streets of the world, with popular places having more detailed maps. It would be great to integrate this information with a AR/VR headset that would allow one to do &lt;a href=&quot;https://www.giroditalia.it/&quot;&gt;&lt;em&gt;Giro d’Italia&lt;/em&gt;&lt;/a&gt; right from one’s living room on a stationary bike. It could even be possible for me to go on a ride with my friends (all of whom are in their stationary bikes probably in their living rooms or patios in different parts of the world), while turning around to talk to them or turning back to check on a slower rider. These require enormous graphics processing power (enabled by GPUs) combined with image processing software (enabled by AI) and a high bandwidth internet connectivity (enabled by 5G). For sports physicians, trainers and training apps this would also enable to monitor their customer’s vital information in real-time as and when they are working out and provide corrective information and guidance.&lt;/p&gt;

&lt;p&gt;There are several business opportunities in the metaverse and several more would emerge. There are several challenges, especially around security of such systems. The success would however down to the right business model as well as focussed, deliberate execution. We might still see several companies getting on the metaverse bandwagon as did several newspaper companies thought internet was just about creating a website without understanding the distribution and personalization powers of the internet. There would be several “me-too” businesses trying to ride the “metaverse hype” without a sustainable business model, while a few who are deliberate would turn out to be the Amazons and Googles of the next decade.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="metaverse" />
      

      

      
        <summary type="html">A previous post outlined several drivers of metaverse. While it is still in its nascent stages, metaverse would bring truly immersive environments whose applications would span beyond video games. With the trends in virtual and augmented reality, there are immense economic opportunities as well as implications. Gaming: The most widely accepted and widely talked about application is, of course, gaming. Game developers have been creating virtual worlds for decades - so what is different now?. With advances in hardware accleration and Deep Learning algorithms, virtual world experience is becoming more and more realistic than in previous generations of games. For example, techniques like real-time ray tracing were known for a very long time. These are beginning to become practical and a commonplace only recently. So are techniques like super sampling techniques using Deep Learning, as well as other compression techniques that enable creating Bounding Volume Hierarchies (BVH) packing more details about the scene. With Deep Learning techniques playing an increasing important role in simulating physics, more interactive and immersive games can be developed that generate close to accurate physics (flames, movement, flow, etc.) making graphics more realistic. There is enough economic opportunity in gaming and with game streaming it has been growing at an accelerated pace. One can check out the Twitch Streaming Statistics and get convinced. Advertising: If I am watching a live sports event on TV (like soccer) from San Francisco, I would see advertisements pertaining to the local (US) market, while the same game, in an Italian feed, will have advertisements tailored to the European market. This is already a well-known and mature technology called virtual replacement perimeter technology. Supponor is one of the leaders in this space. Metaverse would enable to take this a step further. The commercials would not only get specialized or tailored according to the local market where it is being broadcasted live but also according to each viewer’s personal preferences. Much like how Google’s digital advertising and the growth of mobile and social networks allowed personalized, targeted ads, metaverse would offer such opportunities in a more immersive arena. Technologies pioneered by companies like Mirriad for product placement would be extremely valuable and amenable to personalization, not just for live concerts and live sports but many varieties of content created for the metaverse. Of course, there needs to be sufficient guard-rails and transparency and there would also be renewed debates on native advertising. Factory Automation: Like gaming, this is one of the other avenues being talked about. There is one major difference between the virtual world (metaverse) created for gaming and the one created for factory automation purposes. For gaming, the artifacts that make up the metaverse is completely upto the imagination of the creator. However, in the case of factory automation, the digital twin has to replicate a physical structure that already exists. Any change in structure or function created (imagine the direction of conveyor belt in an assembly line) in the physical world has to be reflected in the digital twin. There is also less freedom on what artifacts could to be in the digital twin (metaverse). If many factories have identical assembly lines (like many traditional automobile factories or soft-drink bottling units, etc.) one could work with a templated setup. This would involve creating the artifacts common to all these templates (using Graphics SW tools, videos of factory, AI, etc,.). Further even if many factories share common templated factory floors at a high-level, there might be efficiencies that are introduced at the micro level (kinds of robot-arms they use, kinds of automated inspection mechanisms, etc,.) and these would be the key differentiators leading to competitive advantage. There might be commodity goods made by manufacturers who have no apparent competitive advantage over other similar manufacturers (think of zippers, nails, bottle-caps, automotive brakes and lately some semiconductor components, etc,.) for whom creating a digital twin would be easier. But they are also the ones who would move slowly on such innovations as their business becomes more and more commoditized. On the other hand factories who have a great competitive advantage (in terms of operational efficiencies) can leapfrog by having a centralized view of global factories and bringing them all to the same efficient mode. E-commerce: Advances in technologies like photogrammetry and depth-sensing cameras becoming a commonplace in most smartphones can bring about new innovations in e-commerce. Most of e-commerce today is centered around purchase based on reviews from trying the product offline. It becomes interesting when purchases of clothes and accessories are made as one has to trust the size, fit, etc,. Customers tend to purchase multiple sizes only to return some of them back. Though many never realize, the returns in brick-and-mortar stores is around 8-9% while online retailers can see as much as 30% of the products returned. It runs to billions of dollars worth of merchandise. Virtual trial rooms could pop up at many points of interests (shopping malls, cinemas, etc.) or even in your living room (using a VR head-set and a phone camera or a Kinect system) that would allow one to try out new clothes. It could be tied to other marketing avenues (cross-selling) like selecting a design from a fashion show or a TV series and trying them as and when watching those shows. High-end fashion is already experimenting with augmented reality. Who doesn’t want to go watch virtual French Open sitting next to friends from around the world while wearing your favorite Rafa Nike virutal t-shirt having the Raging Bull logo? Merchandises are already common in many video games and it could seamlessly get into fashion and other virutal events. Given the growth in influencer economy, it would also be an innovative way for amateur fashion designers to showcase their designs and their brand. Healthcare and Bio-informatics: Over the last decade, we have seen increased adoption of robotic surgery - devices like DaVinci systems have been installed in several hospitals around the globe as it minimizes direct human contact during surgery and reduces infection rates. However, Da Vinci is a robotic arm controlled by a skilled surgeon along with mounted cameras and associated image processing software. With decreasing cost of sensors and advancements in software, a sophisticated array of sensors attached to the robotic arm could provide vital information overlaid (augmented reality) on the camera images of the surgery would provide real-time information that could aid in real-time and critical decision making. With advancements in 5G and networking bandwidths, Da Vinci like systems could one day make remote surgeries possible. Here’s where metaverse systems could play a huge role. Effectively collaborating with different expert surgeons connected to the robotic arm from different parts of the globe would require the image processing software to be capable of showing different views, different sets of vital information in real-time to different surgical experts and also keep each of the surgeons informed of what the other surgeon is trying to do. This requires a lot of image processing, processing data from different sensors in real-time, syncronizing the views of different surgeons and doing it all in real-time. Technologies such as these would also play a vital role in medical education, to train more professionals in places where such high-end technologies may not be very common. MRI is another evolving technology with more and more software components, especially image fusion and augmented reality components, along with advances in physics. There is a very nice explanation of MRI in Canon’s webpage. A keen observer would be able to already notice the various places where imaging technology could be improved many folds using techniques like DLSS and the like. Image processing applications in a plethora of fields are getting better using Deep Learning techniques, including such medical imaging systems. Research in genome sequencing has advanced leaps and bounds over the last decade. Whole genome sequencing (WGS) analyses developed using systems like Illumina provide a high-resolution base-by-base view of the genome. Softwares like TruSight combine the features used by different experts all in one dashboard and provide several ways of visualization and analysis. Softwares like these are testament to the convergence of hardware, system software, graphics and image processing and collaboration softwares as well as techniques like machine learning, all of which are required for research in areas like bio-informatics. With metaverse like platforms many experts can tap into the same analysis done in one molecular biology lab, and can sort them in different ways depending on the remote biologist’s expertise. While one might be an expert in variant filtering, the other might be an expert in evidence retrieval. Fitness and Active Sports: Though technologies in fitness got a big boost during the pandemic, mostly due to the interest in exercise equipments like Peloton bikes, sports science has been an active field of research by many global brands like Nike. Companies like Google have already mapped the many streets of the world, with popular places having more detailed maps. It would be great to integrate this information with a AR/VR headset that would allow one to do Giro d’Italia right from one’s living room on a stationary bike. It could even be possible for me to go on a ride with my friends (all of whom are in their stationary bikes probably in their living rooms or patios in different parts of the world), while turning around to talk to them or turning back to check on a slower rider. These require enormous graphics processing power (enabled by GPUs) combined with image processing software (enabled by AI) and a high bandwidth internet connectivity (enabled by 5G). For sports physicians, trainers and training apps this would also enable to monitor their customer’s vital information in real-time as and when they are working out and provide corrective information and guidance. There are several business opportunities in the metaverse and several more would emerge. There are several challenges, especially around security of such systems. The success would however down to the right business model as well as focussed, deliberate execution. We might still see several companies getting on the metaverse bandwagon as did several newspaper companies thought internet was just about creating a website without understanding the distribution and personalization powers of the internet. There would be several “me-too” businesses trying to ride the “metaverse hype” without a sustainable business model, while a few who are deliberate would turn out to be the Amazons and Googles of the next decade.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Metaverse and its Drivers →</title>
      
      <link href="https://dotkay.github.io/2021/10/01/metaverse-drivers/" rel="alternate" type="text/html" title="Metaverse and its Drivers" />
      <published>2021-10-01T00:00:00+00:00</published>
      <updated>2021-10-01T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/10/01/metaverse-drivers</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/10/01/metaverse-drivers/">&lt;p&gt;Lately, we have been hearing a lot about the &lt;em&gt;metaverse&lt;/em&gt;. Although this concept is not new, it has been brought to the limelight because of the huge bets and brand name changes from the potential big players in this space. However, the metaverse that is talked about today is different from what the current state of the internet offers and there are some key technological developments that are drivers of the metaverse and different companies are equipped with different strengths in order to join the competition.&lt;/p&gt;

&lt;p&gt;Avatars and game environments have been around for quite some time, especially in immersive video games. The metaverse is much more than avatars or fancy immersive experiences as in video games. It is a platform, rather than a stand-alone application. It is a platform where several applications can be plugged in and get the benefits of the platform. This might be one of the probable reasons a social media messaging company recently rebranded itself. The new brand could mean a platform much bigger than social media, where social media would be just one of the components that could be plugged in. The metaverse combines AR (Augmented Reality) with communication and collaboration tools and in some cases with the help of VR (Virtual Reality) that results in an immersive, photorealistic communication, collaboration and entertainment experience. Many technological developments that happened over the course of the last decade are key to the big bets on the possibilities of metaverse.&lt;/p&gt;

&lt;p&gt;The main technological advancements could be grouped under the following categories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graphics Hardware (Semiconductors)&lt;/strong&gt;
Computer processors have not just packed more and more transistors but the capabilities of graphics hardware have grown by leaps and bounds the last decade. The semiconductor process technology, new memory technologies, new functions and features driven by deep learning applications, real-time ray tracing, etc. have enabled a new era of computation possibilities at scale that until recently required expensive servers running over long periods of time in order to achieve the same computation. A big part of this is driven by semiconductor process technology (32nm in 2009 to 5nm in 2020) that enabled packing several transistors in a small area (of the die). Similar advances in memory technologies (3D stacked DRAM, advances in DDR as well as HBM and other new memory technologies) helped in increasing the on board storage by a large factor, as required by heavy graphics applications. Similar advances have also resulted in packing enough computation in small form factors like mobile phones, security cameras and other IoT devices. It has resulted in our ability to play games with a reasonably good graphics on our mobile phones. It has also enabled IoT devices to perform data analytics real time as and when they collect sensory data on the field.&lt;/p&gt;

&lt;div class=&quot;img_container&quot;&gt;
  &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/NgcYLIvlp_k&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Graphics Software (VR, AR, Photogrammetry, etc.)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If hardware was all that was needed, it would have been a commodity. The software layer brings in the innovation that software companies constantly compete to better each other at. For example, irrespective of intel or M1 processor the software layer should ensure that the applications work seamlessly across hardware configurations and operating systems. The more tied the hardware is to the software layer, more performance can be squeezed out of the hardware. Many of the graphics applications need a tremendous amount of compute power. This is especially true for photorealistic graphics. Even given the advances in semiconductors, especially graphics processing units, graphics software innovations require a mix of technology, science and art. While most software companies excel at technology, few excel in combining the three. Graphics components are relatively easy to create, but rendering photo-realistic graphics in real time in different form factors is not straight forward. Advances in AI techniques like Deep Learning Super Sampling (&lt;a href=&quot;https://developer.nvidia.com/dlss&quot;&gt;DLSS&lt;/a&gt;) and post-processing techniques like Temporal Anti-Aliasing (&lt;a href=&quot;https://www.nvidia.com/es-la/drivers/txaa-anti-aliasing-technology/&quot;&gt;TAA&lt;/a&gt;) techniques have helped tremendously. Techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Photogrammetry&quot;&gt;photogrammetry&lt;/a&gt; play a vital role in creating digital twins of artifacts, besides 3D cameras and effective and scalable stitching algorithms. AR also involves merging the physical world (camera sensed) augmented with layers of graphics content. Since motion of objects is the undoubted constant in all VR and AR experiences (as anyone who has watched YouTube 360 videos or played with an Oculus headset can attest), simulating physics is the next challenge to overcome. Simulating physics, beyond fire, waves and wind in Computer Graphics is a non-trivial task that requires &lt;a href=&quot;https://www.youtube.com/watch?v=d8yzFfSrDAg&quot;&gt;simulating different materials&lt;/a&gt; like stones, marbles, snow, etc., besides their interactions with light rays (reflection, refraction, etc.) and rendering caustics. Although today’s graphics is a major leap from a decade ago, metaverse would demand a much higher quality of photorealistic real-time graphics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Internet, 5G and streaming games&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Metaverse has the potential to change collaboration and media consumption in ways internet changed marketing and commerce. The advances in the same technologies (web2.0, fiber optics, mobile internet, 3G, 4G) that propelled internet adoption would also catalyze technologies like metaverse. Over the last few decades, internet has completely transformed how we consume media content (streaming media, social media, etc. compared to print and television) and how we collaborate (Webex, Zoom, Google Docs, etc. compared to telephone calls and fax messages) increasing both efficiency and effectiveness. The internet infrastructure already being able to stream online games and movies and constantly improving bandwidth and connectivity, companies can safely bet on further improvements that would support applications in the metaverse. New and upcoming technologies like 5G would bring bandwidth intensive applications like gaming within reach for mobile consumers. As adoption increases there would be new opportunities for businesses with new and innovative business models emerging out of it, much in the same way internet created a wide array of marketplace business models (Amazon, Uber, Doordash, etc.). Many doubted &lt;a href=&quot;https://www.youtube.com/watch?v=GltlJO56S1g&quot;&gt;Amazon’s&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=gipL_CEw-fk&quot;&gt;Microsoft’s&lt;/a&gt; potential at the beginning of the internet era. The companies that do not see the potential of the metaverse might get left behind when the potentials are realized by the leaders.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Social Networks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current decade has seen an interesting growth in content creation. The growth of social media enabled everyone to interact and showcase their talent. Specifically, apps like Facebook, Twitter and TikTok made it possible for independent content creators and small businesses to reach and market their products and services effectively in a cost-efficient manner. Gaming is inherently a social activity. The more immersive or innovative games are, the more attention they get (e.g. &lt;a href=&quot;https://www.inc.com/josh-linkner/the-disruptive-innovation-behind-pok-mon-go.html&quot;&gt;Pokemon Go&lt;/a&gt;). When we think about it, there are many activities like movies, theatre, live sports, education, etc. that are also social activities, or at least the experience is enriched a lot when done as a social activity. Metaverse could offer these activities in the virtual world - for example, right from your living room in Singapore, you could watch US open, sitting in front-row seats at Arthur Ashe stadium, wearing your VR headset sitting next your close friend who is wearing another VR headset and is sitting in his living room in Sydney. Though we might be inclined to think that popular social media players would have an advantage in metaverse, we could also see new innovative players who could change the game. The bottom line is that for many of these social activities, metaverse would offer a much richer experience than what the current internet offers.&lt;/p&gt;

&lt;p&gt;There are many other drivers that might directly or indirectly influence the adoption like enterprise communication and collaboration applications like WebEx or Zoom extended to factory floors rather than just computer screens, virtual tourism and adventure trips, etc. It is difficult to predict if metaverse would succeed but it is definitely going to bring lots of innovations in immersive graphics, simulation as well as hardware architectures.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="metaverse" />
      
        <category term="graphics" />
      
        <category term="semiconductors" />
      
        <category term="software" />
      

      

      
        <summary type="html">Lately, we have been hearing a lot about the metaverse. Although this concept is not new, it has been brought to the limelight because of the huge bets and brand name changes from the potential big players in this space. However, the metaverse that is talked about today is different from what the current state of the internet offers and there are some key technological developments that are drivers of the metaverse and different companies are equipped with different strengths in order to join the competition. Avatars and game environments have been around for quite some time, especially in immersive video games. The metaverse is much more than avatars or fancy immersive experiences as in video games. It is a platform, rather than a stand-alone application. It is a platform where several applications can be plugged in and get the benefits of the platform. This might be one of the probable reasons a social media messaging company recently rebranded itself. The new brand could mean a platform much bigger than social media, where social media would be just one of the components that could be plugged in. The metaverse combines AR (Augmented Reality) with communication and collaboration tools and in some cases with the help of VR (Virtual Reality) that results in an immersive, photorealistic communication, collaboration and entertainment experience. Many technological developments that happened over the course of the last decade are key to the big bets on the possibilities of metaverse. The main technological advancements could be grouped under the following categories. Graphics Hardware (Semiconductors) Computer processors have not just packed more and more transistors but the capabilities of graphics hardware have grown by leaps and bounds the last decade. The semiconductor process technology, new memory technologies, new functions and features driven by deep learning applications, real-time ray tracing, etc. have enabled a new era of computation possibilities at scale that until recently required expensive servers running over long periods of time in order to achieve the same computation. A big part of this is driven by semiconductor process technology (32nm in 2009 to 5nm in 2020) that enabled packing several transistors in a small area (of the die). Similar advances in memory technologies (3D stacked DRAM, advances in DDR as well as HBM and other new memory technologies) helped in increasing the on board storage by a large factor, as required by heavy graphics applications. Similar advances have also resulted in packing enough computation in small form factors like mobile phones, security cameras and other IoT devices. It has resulted in our ability to play games with a reasonably good graphics on our mobile phones. It has also enabled IoT devices to perform data analytics real time as and when they collect sensory data on the field. Graphics Software (VR, AR, Photogrammetry, etc.) If hardware was all that was needed, it would have been a commodity. The software layer brings in the innovation that software companies constantly compete to better each other at. For example, irrespective of intel or M1 processor the software layer should ensure that the applications work seamlessly across hardware configurations and operating systems. The more tied the hardware is to the software layer, more performance can be squeezed out of the hardware. Many of the graphics applications need a tremendous amount of compute power. This is especially true for photorealistic graphics. Even given the advances in semiconductors, especially graphics processing units, graphics software innovations require a mix of technology, science and art. While most software companies excel at technology, few excel in combining the three. Graphics components are relatively easy to create, but rendering photo-realistic graphics in real time in different form factors is not straight forward. Advances in AI techniques like Deep Learning Super Sampling (DLSS) and post-processing techniques like Temporal Anti-Aliasing (TAA) techniques have helped tremendously. Techniques like photogrammetry play a vital role in creating digital twins of artifacts, besides 3D cameras and effective and scalable stitching algorithms. AR also involves merging the physical world (camera sensed) augmented with layers of graphics content. Since motion of objects is the undoubted constant in all VR and AR experiences (as anyone who has watched YouTube 360 videos or played with an Oculus headset can attest), simulating physics is the next challenge to overcome. Simulating physics, beyond fire, waves and wind in Computer Graphics is a non-trivial task that requires simulating different materials like stones, marbles, snow, etc., besides their interactions with light rays (reflection, refraction, etc.) and rendering caustics. Although today’s graphics is a major leap from a decade ago, metaverse would demand a much higher quality of photorealistic real-time graphics. Internet, 5G and streaming games Metaverse has the potential to change collaboration and media consumption in ways internet changed marketing and commerce. The advances in the same technologies (web2.0, fiber optics, mobile internet, 3G, 4G) that propelled internet adoption would also catalyze technologies like metaverse. Over the last few decades, internet has completely transformed how we consume media content (streaming media, social media, etc. compared to print and television) and how we collaborate (Webex, Zoom, Google Docs, etc. compared to telephone calls and fax messages) increasing both efficiency and effectiveness. The internet infrastructure already being able to stream online games and movies and constantly improving bandwidth and connectivity, companies can safely bet on further improvements that would support applications in the metaverse. New and upcoming technologies like 5G would bring bandwidth intensive applications like gaming within reach for mobile consumers. As adoption increases there would be new opportunities for businesses with new and innovative business models emerging out of it, much in the same way internet created a wide array of marketplace business models (Amazon, Uber, Doordash, etc.). Many doubted Amazon’s and Microsoft’s potential at the beginning of the internet era. The companies that do not see the potential of the metaverse might get left behind when the potentials are realized by the leaders. Social Networks The current decade has seen an interesting growth in content creation. The growth of social media enabled everyone to interact and showcase their talent. Specifically, apps like Facebook, Twitter and TikTok made it possible for independent content creators and small businesses to reach and market their products and services effectively in a cost-efficient manner. Gaming is inherently a social activity. The more immersive or innovative games are, the more attention they get (e.g. Pokemon Go). When we think about it, there are many activities like movies, theatre, live sports, education, etc. that are also social activities, or at least the experience is enriched a lot when done as a social activity. Metaverse could offer these activities in the virtual world - for example, right from your living room in Singapore, you could watch US open, sitting in front-row seats at Arthur Ashe stadium, wearing your VR headset sitting next your close friend who is wearing another VR headset and is sitting in his living room in Sydney. Though we might be inclined to think that popular social media players would have an advantage in metaverse, we could also see new innovative players who could change the game. The bottom line is that for many of these social activities, metaverse would offer a much richer experience than what the current internet offers. There are many other drivers that might directly or indirectly influence the adoption like enterprise communication and collaboration applications like WebEx or Zoom extended to factory floors rather than just computer screens, virtual tourism and adventure trips, etc. It is difficult to predict if metaverse would succeed but it is definitely going to bring lots of innovations in immersive graphics, simulation as well as hardware architectures.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Stories in Charts →</title>
      
      <link href="https://dotkay.github.io/2021/09/22/stories-in-charts/" rel="alternate" type="text/html" title="Stories in Charts" />
      <published>2021-09-22T00:00:00+00:00</published>
      <updated>2021-09-22T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/09/22/stories-in-charts</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/09/22/stories-in-charts/">&lt;p&gt;In this post, we will see the most ubiquitous, most useful and also the most easily mis-interpreted data visualization tools - &lt;em&gt;charts&lt;/em&gt;. They come in various forms and they are used to represent different aspects of the underlying data. The key assumption is that the visual representation is easier to interpret quickly than in a tabular format. More often, one kind of story a chart conveys might hide another kind of story a different chart, of the underlying dataset, might have conveyed. It is not uncommon to find that the reader would interpret the chart in a way different from what the writer wanted to convey, in which case it is best for the writer to clarify the message being driven.&lt;/p&gt;

&lt;p&gt;For example, let us look at the latest global vaccination data. When you look at the per-capita vaccination rates of different countries, and say you pick the top 50 countries, you would see a chart similar to the following (the chart below is from September, 2021). More specifically, the chart shows the number of fully vaccinated people per hundred of the population.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/people_vax_per_100.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;What story does this tell? While per-capita numbers are a way to normalize the data, it crucially depends on the story that one wants to tell. Does this indicate the ease of availability of vaccines? Does it indicate the effectiveness of the healthcare system? Does this indicate the willingness of people to get the vaccine? Or something else?&lt;/p&gt;

&lt;p&gt;Let us look at the absolute numbers for the same data and look at the chart.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/people_vax.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span style=&quot;color:gray; font-size: 50%; text-align: left;&quot;&gt;
Data regarding fully vaccinated population from China was missing from the dataset and hence it does not show up in the chart. China had the most fully vaccinated population (absolute numbers) in September, 2021.
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We see a different picture. What story does this tell?&lt;/p&gt;

&lt;p&gt;While both of the above charts convey useful information, it is necessary to understand what useful story could be told using the above. For example, the first chart (per-capita) could indicate what percentage of the total population is vaccinated. This could indicate to what extent the spread of the virus or hospitalizations could be contained and that very likely these countries could get out of the pandemic sooner and become safer to travel to. It does not indicate the effectiveness of the healthcare system or the challenges in getting more people vaccinated, etc.&lt;/p&gt;

&lt;p&gt;The second chart shows the absolute number of people vaccinated in many countries. The difference between the two charts could show the challenges in getting the population vaccinated - it could be geographical as in some countries might be too large posing a logistics challenge, it could be the size of the population - even if enough vaccines are in supply more would be needed to get the entire population vaccinated or it could be a short supply of vaccines - again because of lack of storage or the price or a weak healthcare infrastructure.&lt;/p&gt;

&lt;p&gt;On similar lines, when you look at GDP data and plot the top 50 countries by GDP the chart looks something like the following.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/gdp_2018_top_50.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span style=&quot;color:gray; font-size: 50%; text-align: left;&quot;&gt;
The GDP numbers are from 2018 rather than latest numbers in order to avoid the variations of the effects of the pandemic on GDP.
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It shows the wealthiest countries (in terms of GDP). But does the wealth trickle down to every individual? Again, there might be challenges like population, management of funds, etc. Once you look at per-capita numbers one could see how different countries fare.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/gdp_per_capita_2018_top_50.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;When you look at the GDP growth rate, it tells a different story - the countries that are improving from their past year. Yes, Libya did have 15% GDP growth in 2018.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/gdp_growth_rate_2018.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The chart we choose should depend on the story we want to tell. It is imperative to explicitly explain the choice of chart and how it tells the story the writer wants to tell and also the stories it does not.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray; font-size: 70%; text-align: left;&quot;&gt;
The vaccination data used here is from a &lt;a href=&quot;https://www.kaggle.com/gpreda/covid-world-vaccination-progress&quot; target=&quot;_self&quot;&gt;Kaggle dataset&lt;/a&gt;.
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="vaccination" />
      
        <category term="gdp" />
      
        <category term="per-capita" />
      
        <category term="charts" />
      
        <category term="data science" />
      

      

      
        <summary type="html">In this post, we will see the most ubiquitous, most useful and also the most easily mis-interpreted data visualization tools - charts. They come in various forms and they are used to represent different aspects of the underlying data. The key assumption is that the visual representation is easier to interpret quickly than in a tabular format. More often, one kind of story a chart conveys might hide another kind of story a different chart, of the underlying dataset, might have conveyed. It is not uncommon to find that the reader would interpret the chart in a way different from what the writer wanted to convey, in which case it is best for the writer to clarify the message being driven. For example, let us look at the latest global vaccination data. When you look at the per-capita vaccination rates of different countries, and say you pick the top 50 countries, you would see a chart similar to the following (the chart below is from September, 2021). More specifically, the chart shows the number of fully vaccinated people per hundred of the population. What story does this tell? While per-capita numbers are a way to normalize the data, it crucially depends on the story that one wants to tell. Does this indicate the ease of availability of vaccines? Does it indicate the effectiveness of the healthcare system? Does this indicate the willingness of people to get the vaccine? Or something else? Let us look at the absolute numbers for the same data and look at the chart. Data regarding fully vaccinated population from China was missing from the dataset and hence it does not show up in the chart. China had the most fully vaccinated population (absolute numbers) in September, 2021. We see a different picture. What story does this tell? While both of the above charts convey useful information, it is necessary to understand what useful story could be told using the above. For example, the first chart (per-capita) could indicate what percentage of the total population is vaccinated. This could indicate to what extent the spread of the virus or hospitalizations could be contained and that very likely these countries could get out of the pandemic sooner and become safer to travel to. It does not indicate the effectiveness of the healthcare system or the challenges in getting more people vaccinated, etc. The second chart shows the absolute number of people vaccinated in many countries. The difference between the two charts could show the challenges in getting the population vaccinated - it could be geographical as in some countries might be too large posing a logistics challenge, it could be the size of the population - even if enough vaccines are in supply more would be needed to get the entire population vaccinated or it could be a short supply of vaccines - again because of lack of storage or the price or a weak healthcare infrastructure. On similar lines, when you look at GDP data and plot the top 50 countries by GDP the chart looks something like the following. The GDP numbers are from 2018 rather than latest numbers in order to avoid the variations of the effects of the pandemic on GDP. It shows the wealthiest countries (in terms of GDP). But does the wealth trickle down to every individual? Again, there might be challenges like population, management of funds, etc. Once you look at per-capita numbers one could see how different countries fare. When you look at the GDP growth rate, it tells a different story - the countries that are improving from their past year. Yes, Libya did have 15% GDP growth in 2018. The chart we choose should depend on the story we want to tell. It is imperative to explicitly explain the choice of chart and how it tells the story the writer wants to tell and also the stories it does not. The vaccination data used here is from a Kaggle dataset.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">The Stories of Summer Olympics →</title>
      
      <link href="https://dotkay.github.io/2021/08/16/stories-of-summer-olympics/" rel="alternate" type="text/html" title="The Stories of Summer Olympics" />
      <published>2021-08-16T00:00:00+00:00</published>
      <updated>2021-08-16T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/08/16/stories-of-summer-olympics</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/08/16/stories-of-summer-olympics/">&lt;p&gt;This is about the stories of Summer Olympics - not just one story but many stories and not just the recently concluded Tokyo Olympics, but all of the Olympic games since the first one in Athens in 1896. Although the analysis of the medals won by different countries would make the story read like a story medals and achievements statistics, there are other stories hidden within. We will narrate many of those as we do the analysis.&lt;/p&gt;

&lt;p&gt;We will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt; library to do the data analysis and plotly to create our plots that would tell a visual story.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.express&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When we look at the hosts we see many western powers with Brazil and China (then emerging economies) hosting in 2008 and 2016 respectively. Of course, hosting an Olympics is no small thing - the infrastructure, safety, accessibility, etc. play a huge role and many of these cost money. London seems to be the only city that has hosted Olympics thrice.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;total_medals_by_host_city&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summer_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'City'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_medals_by_host_city&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'City'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;hover_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medals Awarded at Each Venue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_annotation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'London'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1690&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;London has hosted Olympics thrice...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;opacity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The number of events in each of the Olympics also seems to be different, as evidenced by the total number of medals awarded during the games. With Paris and London regularly competing for the best European City regularly I believe it is high time Paris got a chance to host the Summer Olympic games, and very glad 2024 Olympics are going to be held in Paris. I also hope more and more emerging economies to be able to host the games. The motivation and interest in sports such events would infuse is invaluable.&lt;/p&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/17.embed&quot; height=&quot;525&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;When we look at the cumulative number of medals won from the start of Summer Olympic games, USA clearly leads. But more important to note here is Russia which was more excellent in sports, science and technology, arts and culture, etc., when it was Soviet Union. Also interesting to note is the presence of an entry &lt;em&gt;Individual athletes&lt;/em&gt; - athletes who chose to compete under the Olympic flag instead of any specific country. This is a political story.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;total_medals_by_country&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summer_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_medals_by_country&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;hover_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cumulative Number of Medals Won (1896-2021)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There were many instances where athletes who requested to compete independently and were refused - for example, the ones who emigrated from Communist Europe during the cold war, as well as some during the &lt;a href=&quot;https://en.wikipedia.org/wiki/1976_Olympic_boycott&quot;&gt;1976 Olympic boycott&lt;/a&gt;. The first provisions were made by IOC (International Olympic Committee) during the 1980 Summer Olympics hosted at Moscow.&lt;/p&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; align=&quot;middle&quot; src=&quot;https://plotly.com/~kkplot/15.embed&quot; height=&quot;600&quot; width=&quot;1000&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;When we look at the countries that shine on the top of the medals table regularly the familiar pattern is clear. It is mostly European countries, USA, Australia and more recently China. Although we might have cheered Usain Bolt’s sprints and track and field athletics being one of the most anticipated events of the Summer Olympic games, in terms of total number of medals, many western countries top the table year after year. This also indicates the broad variety and the number of events in the summer games.&lt;/p&gt;

&lt;p&gt;There is also a political story hidden here, that of the 1980 Summer Olympics. There were only two cities that bid to host the games - Los Angeles and Moscow and eventually Moscow won. If you notice the 1980 Olympics that was hosted by Moscow, Russia seems to be the clear leader in terms of number of medals. Almost always, one expects host country to win several medals, but scoring more medals than the usual competitive powers is always a big achievement. Many nations boycotted the 1980 Olympics, led by the USA to protest against the Soviet-Afghan war. It had the smallest participation - just around 80 countries.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;top_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; 
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Country&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Medal Count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sum&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Medal Count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summer_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Medal Count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Country&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; 
             &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal Count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;hover_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Medal Count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Medal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;barmode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'stack'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;color_continuous_scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Viridis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Medal Winners in Olympics'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;iframe id=&quot;igraph&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/19.embed&quot; height=&quot;800&quot; width=&quot;1000&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;There are more politics stories in Olympics. The apparent gaps in the plot below makes one wonder what happened those years. The 1916 summer Olympics, to be hosted by Berlin, were cancelled due to World War I, a first cancellation in the 20 year history of Olympics during those days. The 1940 Olympics to be hosted by Tokyo were rescheduled to be hosted by Finland (due to Japan’s invasion of China in 1937) and eventually cancelled due to the outbreak of World War II. Again, the 1944 Summer Olympics to be hosted by London got cancelled due to World War II. The 2020 Olympics hosted by Tokyo was rescheduled to 2021 due to the outbreak of Covid. It took tremendous courage and organization by Japan and the participating countries to put up a great show during the pandemic.&lt;/p&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/21.embed&quot; height=&quot;800&quot; width=&quot;1000&quot;&gt;&lt;/iframe&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_final&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fillna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idcols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idcols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Cumulative Total Medals'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dfnoc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'NOC'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, to finish it off, let us see how the top 25 medal winners (measured as per medals won in 2020)
have fared over the years in terms of total medals won. The rise of China as a major sporting nation is very apparent.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_final&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_final&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Country&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Country&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Cumulative Total Medals&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Cumulative Total Medals&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color_discrete_sequence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;brown&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gold&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;silver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Medal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;animation_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Year&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hover_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Country&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The Olympics Story&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;iframe id=&quot;igraph&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/24.embed&quot; height=&quot;800&quot; width=&quot;1100&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;While this short post tells several stories behind the Summer Olympics, I would like to write a future post on the stories behind Paralympics which will would be stories of courge, inspiration and extraordinary commitment.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray; font-size: 70%; text-align: left;&quot;&gt;
The data used here is from a &lt;a href=&quot;https://www.kaggle.com/amirba/olympic-sports-and-medals-18962021&quot; target=&quot;_self&quot;&gt;Kaggle dataset&lt;/a&gt;. This &lt;a href=&quot;https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results&quot; target=&quot;_self&quot;&gt;Kaggle dataset&lt;/a&gt; was also used to get NOC (National Olympic Committee) codes of the different countries. Some of the numbers in the dataset does not seem to agree with tables in Wikipedia, nevertheless the differences are not huge to change the story.
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="Olympics" />
      
        <category term="medals" />
      
        <category term="data science" />
      

      

      
        <summary type="html">This is about the stories of Summer Olympics - not just one story but many stories and not just the recently concluded Tokyo Olympics, but all of the Olympic games since the first one in Athens in 1896. Although the analysis of the medals won by different countries would make the story read like a story medals and achievements statistics, there are other stories hidden within. We will narrate many of those as we do the analysis. We will use pandas library to do the data analysis and plotly to create our plots that would tell a visual story. import pandas as pd import plotly.express as px When we look at the hosts we see many western powers with Brazil and China (then emerging economies) hosting in 2008 and 2016 respectively. Of course, hosting an Olympics is no small thing - the infrastructure, safety, accessibility, etc. play a huge role and many of these cost money. London seems to be the only city that has hosted Olympics thrice. total_medals_by_host_city = summer_df.groupby(['City', 'Year'], as_index=False)['Medal'].count() fig = px.bar(total_medals_by_host_city, x='City', y='Medal', hover_data = ['Medal', 'Year'], color='Medal', title='Medals Awarded at Each Venue') fig.add_annotation(x='London', y=1690, text=&quot;London has hosted Olympics thrice...&quot;, ... ... opacity=1.0 ) The number of events in each of the Olympics also seems to be different, as evidenced by the total number of medals awarded during the games. With Paris and London regularly competing for the best European City regularly I believe it is high time Paris got a chance to host the Summer Olympic games, and very glad 2024 Olympics are going to be held in Paris. I also hope more and more emerging economies to be able to host the games. The motivation and interest in sports such events would infuse is invaluable. When we look at the cumulative number of medals won from the start of Summer Olympic games, USA clearly leads. But more important to note here is Russia which was more excellent in sports, science and technology, arts and culture, etc., when it was Soviet Union. Also interesting to note is the presence of an entry Individual athletes - athletes who chose to compete under the Olympic flag instead of any specific country. This is a political story. total_medals_by_country = summer_df.groupby('Country', as_index=False)['Medal'].count() fig = px.bar(total_medals_by_country, x='Country', y='Medal', hover_data = ['Medal'], color='Medal', title='Cumulative Number of Medals Won (1896-2021)') There were many instances where athletes who requested to compete independently and were refused - for example, the ones who emigrated from Communist Europe during the cold war, as well as some during the 1976 Olympic boycott. The first provisions were made by IOC (International Olympic Committee) during the 1980 Summer Olympics hosted at Moscow. When we look at the countries that shine on the top of the medals table regularly the familiar pattern is clear. It is mostly European countries, USA, Australia and more recently China. Although we might have cheered Usain Bolt’s sprints and track and field athletics being one of the most anticipated events of the Summer Olympic games, in terms of total number of medals, many western countries top the table year after year. This also indicates the broad variety and the number of events in the summer games. There is also a political story hidden here, that of the 1980 Summer Olympics. There were only two cities that bid to host the games - Los Angeles and Moscow and eventually Moscow won. If you notice the 1980 Olympics that was hosted by Moscow, Russia seems to be the clear leader in terms of number of medals. Almost always, one expects host country to win several medals, but scoring more medals than the usual competitive powers is always a big achievement. Many nations boycotted the 1980 Olympics, led by the USA to protest against the Soviet-Afghan war. It had the smallest participation - just around 80 countries. def top_n(n): return ( df .groupby(&quot;Country&quot;) .agg({&quot;Medal Count&quot;: &quot;sum&quot;}) .sort_values(&quot;Medal Count&quot;, ascending=False) .head(n) ) df = summer_df.groupby(['Year', 'Country', 'Medal']).size().reset_index() df.columns = ['Year', 'Country', 'Medal', 'Medal Count'] fig = px.bar(df.loc[df[&quot;Country&quot;].isin(top_n(10).index)], x='Year', y='Medal Count', hover_data = ['Country', 'Medal Count', 'Medal'], color='Country', barmode='stack', color_continuous_scale=px.colors.sequential.Viridis, title='Medal Winners in Olympics') There are more politics stories in Olympics. The apparent gaps in the plot below makes one wonder what happened those years. The 1916 summer Olympics, to be hosted by Berlin, were cancelled due to World War I, a first cancellation in the 20 year history of Olympics during those days. The 1940 Olympics to be hosted by Tokyo were rescheduled to be hosted by Finland (due to Japan’s invasion of China in 1937) and eventually cancelled due to the outbreak of World War II. Again, the 1944 Summer Olympics to be hosted by London got cancelled due to World War II. The 2020 Olympics hosted by Tokyo was rescheduled to 2021 due to the outbreak of Covid. It took tremendous courage and organization by Japan and the participating countries to put up a great show during the pandemic. df_final = (df).fillna(0).sort_values(idcols) .groupby(idcols[1:]) .cumsum() .rename(columns={0: 'Cumulative Total Medals'}) .reset_index() .merge(dfnoc, on='NOC') Now, to finish it off, let us see how the top 25 medal winners (measured as per medals won in 2020) have fared over the years in terms of total medals won. The rise of China as a major sporting nation is very apparent. fig = px.scatter( df_final.loc[df_final[&quot;Country&quot;].isin(top_n(25).index)], x=&quot;Country&quot;, y=&quot;Cumulative Total Medals&quot;, log_y=True, size=&quot;Cumulative Total Medals&quot;, color_discrete_sequence=[&quot;brown&quot;,&quot;gold&quot;,&quot;silver&quot;], color=&quot;Medal&quot;, animation_frame=&quot;Year&quot;, hover_name=&quot;Country&quot;, title=&quot;The Olympics Story&quot;, ) While this short post tells several stories behind the Summer Olympics, I would like to write a future post on the stories behind Paralympics which will would be stories of courge, inspiration and extraordinary commitment. The data used here is from a Kaggle dataset. This Kaggle dataset was also used to get NOC (National Olympic Committee) codes of the different countries. Some of the numbers in the dataset does not seem to agree with tables in Wikipedia, nevertheless the differences are not huge to change the story.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">More Software and Silicon than Steel →</title>
      
      <link href="https://dotkay.github.io/2021/07/07/software-silicon-steel/" rel="alternate" type="text/html" title="More Software and Silicon than Steel" />
      <published>2021-07-07T00:00:00+00:00</published>
      <updated>2021-07-07T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/07/07/software-silicon-steel</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/07/07/software-silicon-steel/">&lt;p&gt;Automakers are in the news lately, not for the the usual reasons like launching fully electric models of their vehicles, or factory closures or emission scandals. They are in the news because of semiconductor shortage, throwing many into a state of shock. For people following the auto industry this is only as common as &lt;a href=&quot;https://www.techradar.com/news/chip-maker-for-apple-qualcomm-amd-warns-chip-shortage-to-last-through-2022&quot;&gt;Apple, Qualcomm or AMD’s woes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The rise of the ECUs&lt;/strong&gt;
For more than a decade now, there has been more and more software and silicon that go into automobiles than steel. A typical automobile has about a couple of hundred Electronic Control Units (ECUs) which are microcontrollers each receiving inputs from different (like tire pressure, oil, seat belt, engine temperature, etc.) and producing outputs that translate to action (like a dashboard indicator light on low tire pressure, etc.). Since the cost of sensors have been going down rapidly, more and more of them are added in today’s vehicles and hence more ECUs to process them. The software that goes along is also tightly coupled with the hardware and these ECUs are bought from different OEMs. Much like a computer architecture, most of the automakers connect these ECUs through a common bus usually over a Controller Area Network &lt;a href=&quot;https://www.engineersgarage.com/can-protocol-understanding-the-controller-area-network-protocol/&quot;&gt;(CAN)&lt;/a&gt;. The number of ECUs have only been increasing with the number of features. Rain sensors, daylight sensors, blind-spot sensors, parking sensors are all commonplace in today’s vehicles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/tmp/main/misc/auto_ecus.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span style=&quot;color:gray; font-size: 60%;&quot;&gt;
Source: How Computer Science Helps Car Technology - Bruce Yellin
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;From ECUs to DCUs&lt;/strong&gt;
ECUs constitute a &lt;em&gt;decentralized&lt;/em&gt; architecture - each ECU controls a particular function. With the increasing number of ECUs controlling critical functions like ADAS (Advanced Driver Assist Systems), autonomous driving, radars, etc., the industry has been moving to an architecture called DCU (Domain Control Unit) wherein instead of hundreds of ECUs a few DCUs would control multiple functions. This has been enabled by advances in semiconductors (performance and power) over the years and the decreasing cost of sensors. In a sense, DCUs constitute a &lt;em&gt;centralized&lt;/em&gt; architecture. Much like how a PC Operating System can make way for high priority tasks, the software enabling the DCUs could prioritize critical functions. Further, this also enables flexibility and extensibility. The hardware (sensors and processing power) would remain in the vehicle and new software when installed would provide a new feature, much like how you have a CPU and memory (hardware) in your laptop and you can install Microsoft Office (software) if and when needed. From CAN, the automobiles are trying to move on to ethernet based architecture for many of the functionalities because of the faster transmission speed and more data per frame. CAN provides predictable and collision-free network access between nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Into the Future&lt;/strong&gt;
With advanced sensors (like cameras and lidar) providing streaming, high resolution data, the processing power of the hardware needs to be beefed up in order to process them in real-time. This has enabled the makers of high performance processors to enter the market to compete with the many ECU vendors. NVIDIA and Intel are two of the major processor makers that are making their presence felt in the automotive space, besides Qualcomm and others. With hardware being the muscle, more and more value would be added by innovation in software (the mind). For example, a camera might just send frames of a snowy scene which software could then process, along with some real-time data it receives from neibouring vehicles or cellular phone towers, etc. and create an actionable insight for the actuators. Software can use the historical data from the vehicle for diagnosis and more importantly prognosis of faults. With the decreasing cost of sensors the data collected will explode in volume over time and the automotive software stack might also include a cloud component to store data for continuous processing.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As the importance of electronics and software has grown, so has complexity. Take the exploding number of software lines of code (SLOC) contained in modern cars as an example. In 2010, some vehicles had about ten million SLOC; by 2016, this expanded by a factor of 15, to roughly 150 million lines. Snowballing complexity is causing significant software-related quality issues, as evidenced by millions of recent vehicle recalls - &lt;a href=&quot;https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/rethinking-car-software-and-electronics-architecture&quot;&gt;McKinsey &amp;amp; Co.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With most vehicles made of steel and aluminium the value differentiator was brought about by the (electronic) features over the past decade. The next decade would usher in a new era of value differentiation enabled by software innovations. Who would own the hardware and operating system (much like Windows and Intel or Android and ARM in PC and mobile generation) of the era of automobile innovation is yet to be seen.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We really designed the Model S to be a very sophisticated computer on wheels 
– Elon Musk, 2015&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      

      
        <category term="automotive" />
      
        <category term="semiconductors" />
      
        <category term="software" />
      
        <category term="IoT" />
      

      

      
        <summary type="html">Automakers are in the news lately, not for the the usual reasons like launching fully electric models of their vehicles, or factory closures or emission scandals. They are in the news because of semiconductor shortage, throwing many into a state of shock. For people following the auto industry this is only as common as Apple, Qualcomm or AMD’s woes. The rise of the ECUs For more than a decade now, there has been more and more software and silicon that go into automobiles than steel. A typical automobile has about a couple of hundred Electronic Control Units (ECUs) which are microcontrollers each receiving inputs from different (like tire pressure, oil, seat belt, engine temperature, etc.) and producing outputs that translate to action (like a dashboard indicator light on low tire pressure, etc.). Since the cost of sensors have been going down rapidly, more and more of them are added in today’s vehicles and hence more ECUs to process them. The software that goes along is also tightly coupled with the hardware and these ECUs are bought from different OEMs. Much like a computer architecture, most of the automakers connect these ECUs through a common bus usually over a Controller Area Network (CAN). The number of ECUs have only been increasing with the number of features. Rain sensors, daylight sensors, blind-spot sensors, parking sensors are all commonplace in today’s vehicles. Source: How Computer Science Helps Car Technology - Bruce Yellin From ECUs to DCUs ECUs constitute a decentralized architecture - each ECU controls a particular function. With the increasing number of ECUs controlling critical functions like ADAS (Advanced Driver Assist Systems), autonomous driving, radars, etc., the industry has been moving to an architecture called DCU (Domain Control Unit) wherein instead of hundreds of ECUs a few DCUs would control multiple functions. This has been enabled by advances in semiconductors (performance and power) over the years and the decreasing cost of sensors. In a sense, DCUs constitute a centralized architecture. Much like how a PC Operating System can make way for high priority tasks, the software enabling the DCUs could prioritize critical functions. Further, this also enables flexibility and extensibility. The hardware (sensors and processing power) would remain in the vehicle and new software when installed would provide a new feature, much like how you have a CPU and memory (hardware) in your laptop and you can install Microsoft Office (software) if and when needed. From CAN, the automobiles are trying to move on to ethernet based architecture for many of the functionalities because of the faster transmission speed and more data per frame. CAN provides predictable and collision-free network access between nodes. Into the Future With advanced sensors (like cameras and lidar) providing streaming, high resolution data, the processing power of the hardware needs to be beefed up in order to process them in real-time. This has enabled the makers of high performance processors to enter the market to compete with the many ECU vendors. NVIDIA and Intel are two of the major processor makers that are making their presence felt in the automotive space, besides Qualcomm and others. With hardware being the muscle, more and more value would be added by innovation in software (the mind). For example, a camera might just send frames of a snowy scene which software could then process, along with some real-time data it receives from neibouring vehicles or cellular phone towers, etc. and create an actionable insight for the actuators. Software can use the historical data from the vehicle for diagnosis and more importantly prognosis of faults. With the decreasing cost of sensors the data collected will explode in volume over time and the automotive software stack might also include a cloud component to store data for continuous processing. As the importance of electronics and software has grown, so has complexity. Take the exploding number of software lines of code (SLOC) contained in modern cars as an example. In 2010, some vehicles had about ten million SLOC; by 2016, this expanded by a factor of 15, to roughly 150 million lines. Snowballing complexity is causing significant software-related quality issues, as evidenced by millions of recent vehicle recalls - McKinsey &amp;amp; Co. With most vehicles made of steel and aluminium the value differentiator was brought about by the (electronic) features over the past decade. The next decade would usher in a new era of value differentiation enabled by software innovations. Who would own the hardware and operating system (much like Windows and Intel or Android and ARM in PC and mobile generation) of the era of automobile innovation is yet to be seen. We really designed the Model S to be a very sophisticated computer on wheels – Elon Musk, 2015</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">What have people been watching on YouTube? →</title>
      
      <link href="https://dotkay.github.io/2021/06/06/us-youtube-views/" rel="alternate" type="text/html" title="What have people been watching on YouTube?" />
      <published>2021-06-06T00:00:00+00:00</published>
      <updated>2021-06-06T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/06/06/us-youtube-views</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/06/06/us-youtube-views/">&lt;p&gt;For more than 15 months people around the world are mostly staying indoors, those who could are working from home, vacations and other outdoor entertainments have come to a standstill. At the same time, NetFlix, gaming, and other entertainment like TV and &lt;a href=&quot;https://www.youtube.com/&quot;&gt;YouTube&lt;/a&gt; must have become more popular - after all, these are the forms of indoor entertainment. I decided to look at what people have been interested within this limited set of entertainment avenues. While I could not find most data, YouTube data was available and regularly updated. I decided to download the dataset that contains YouTube data from August 2020 until early June 2021. Let us see what people (in the U.S.) have been watching on YouTube during this period.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US_youtube_trending_data.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yt_categories&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US_category_id.json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtypes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The different features in the dataset are (after casting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;publishedAt&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trending_date&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datetime&lt;/code&gt; type):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;video_id                          object
title                             object
publishedAt          datetime64[ns, UTC]
channelId                         object
channelTitle                      object
categoryId                        object
trending_date        datetime64[ns, UTC]
tags                              object
view_count                         int64
likes                              int64
dislikes                           int64
comment_count                      int64
thumbnail_link                    object
comments_disabled                   bool
ratings_disabled                    bool
description                       object
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are different categories of videos in the dataset, and these are the number of videos in each category.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/n_cat_videos.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;I am surpised by the number of videos in some of the categories or the categorization itself. There is a category on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;People &amp;amp; Blogs&lt;/code&gt; - I am not sure what it refers to. I do know artists have their own channels and YouTube is a more lively form of a blog, but there is also Music (some of the channels there might also be by music artists). While I try and understand how the different videos are categorized, let us look at some other stats.&lt;/p&gt;

&lt;p&gt;Let us find out how many aggregate views each of the different categories has:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cat_view_count_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categoryId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_view_count_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categoryId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Set2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_view_count_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/nviews_cat_videos.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Not surprisingly, Music, Entertainment and Gaming are right at the top. Unfortunately, Education category is way behind. I find YouTube as a wonderful source of education. But may be this is due to Coursera, Udacity, etc. having their own video encoders and decoders and do not have their content on YouTube.&lt;/p&gt;

&lt;p&gt;Let us see which were the Top 50 most viewed videos during the time period, in the U.S.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# create a new column 'count_max_view' to collect max view_count
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'count_max_view'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'video_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# sort the column so that we can pick out the top 50
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top50&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'count_max_view'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'count_max_view'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                                                                          &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# remove non alpha-numeric characters from the titles
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'[^a-zA-Z0-9]'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And let us plot the dataset to see which ones are on the top:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;catplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'muted'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;aspect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/top50_yt_videos.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Since I am fond of Education, I would like to see which YouTube channels have maxinum number of views and check if any of my favourite channels appear in the list.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;yt_edu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt_us&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categoryId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Education'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yt_edu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_edu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'channelTitle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt_edu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'channelTitle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Set2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt_edu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'view_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channelTitle&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/yt_edu_views.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;I see a lot of interesting channels - Khan Academy seems to have just made it to the list. I also see a channel &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YouTube&lt;/code&gt;. I see lots of interesting channel names and I hope to spend more time exploring those on YouTube very soon.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.kaggle.com/rsrishav/youtube-trending-video-dataset&quot;&gt;YouTube dataset&lt;/a&gt; is also available for other geographies. Someday, I would like to compare how the interests in different geographies differ by comparing YouTube channels’ popularity. It may not be an accurate representation of interests at all, but it should be a fun exercise.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="US" />
      
        <category term="YouTube" />
      
        <category term="Education" />
      
        <category term="Pandemic" />
      

      

      
        <summary type="html">For more than 15 months people around the world are mostly staying indoors, those who could are working from home, vacations and other outdoor entertainments have come to a standstill. At the same time, NetFlix, gaming, and other entertainment like TV and YouTube must have become more popular - after all, these are the forms of indoor entertainment. I decided to look at what people have been interested within this limited set of entertainment avenues. While I could not find most data, YouTube data was available and regularly updated. I decided to download the dataset that contains YouTube data from August 2020 until early June 2021. Let us see what people (in the U.S.) have been watching on YouTube during this period. yt_us = pd.read_csv('US_youtube_trending_data.csv') yt_categories = pd.read_json('US_category_id.json') yt_us.dtypes The different features in the dataset are (after casting publishedAt and trending_date to datetime type): video_id object title object publishedAt datetime64[ns, UTC] channelId object channelTitle object categoryId object trending_date datetime64[ns, UTC] tags object view_count int64 likes int64 dislikes int64 comment_count int64 thumbnail_link object comments_disabled bool ratings_disabled bool description object There are different categories of videos in the dataset, and these are the number of videos in each category. I am surpised by the number of videos in some of the categories or the categorization itself. There is a category on People &amp;amp; Blogs - I am not sure what it refers to. I do know artists have their own channels and YouTube is a more lively form of a blog, but there is also Music (some of the channels there might also be by music artists). While I try and understand how the different videos are categorized, let us look at some other stats. Let us find out how many aggregate views each of the different categories has: cat_view_count_df = yt_us.groupby(['categoryId'])['view_count'].agg(['sum']) chart = sns.barplot( data=cat_view_count_df, x='categoryId', y='view_count', palette='Set2', order=cat_view_count_df.sort_values('view_count', ascending=False).category ) Not surprisingly, Music, Entertainment and Gaming are right at the top. Unfortunately, Education category is way behind. I find YouTube as a wonderful source of education. But may be this is due to Coursera, Udacity, etc. having their own video encoders and decoders and do not have their content on YouTube. Let us see which were the Top 50 most viewed videos during the time period, in the U.S. # create a new column 'count_max_view' to collect max view_count yt_us['count_max_view'] = yt_us.groupby(['video_id'])['view_count'].transform(max) # sort the column so that we can pick out the top 50 top50 = yt_us[yt_us['view_count'] == yt_us['count_max_view']].sort_values(by=['count_max_view'], ascending=False).head(50) # remove non alpha-numeric characters from the titles top50.title = top50.title.str.replace('[^a-zA-Z0-9]', '') And let us plot the dataset to see which ones are on the top: fig = sns.catplot( data=top50, x='title', y='view_count', kind='bar', palette='muted', aspect=2, legend=False ) Since I am fond of Education, I would like to see which YouTube channels have maxinum number of views and check if any of my favourite channels appear in the list. yt_edu = yt_us[yt_us['categoryId'] == 'Education'] yt_edu = yt_edu.groupby(['channelTitle'])['view_count'].agg(['sum']) chart = sns.barplot( data=yt_edu, x='channelTitle', y='view_count', palette='Set2', order=yt_edu.sort_values('view_count', ascending=False).channelTitle ) I see a lot of interesting channels - Khan Academy seems to have just made it to the list. I also see a channel YouTube. I see lots of interesting channel names and I hope to spend more time exploring those on YouTube very soon. The YouTube dataset is also available for other geographies. Someday, I would like to compare how the interests in different geographies differ by comparing YouTube channels’ popularity. It may not be an accurate representation of interests at all, but it should be a fun exercise.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">U.S. - Job Losses and Recovery →</title>
      
      <link href="https://dotkay.github.io/2021/03/15/us-job-loss-and-recovery/" rel="alternate" type="text/html" title="U.S. - Job Losses and Recovery" />
      <published>2021-03-15T00:00:00+00:00</published>
      <updated>2021-03-15T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/03/15/us-job-loss-and-recovery</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/03/15/us-job-loss-and-recovery/">&lt;p&gt;In a &lt;a href=&quot;https://dotkay.github.io/2021/02/21/us-unemployment&quot;&gt;previous post&lt;/a&gt;, we saw how many jobs the different states in the U.S. have lost over the years primarily due to trade deficits. In this post, we will visually analyze the &lt;a href=&quot;https://www.bls.gov/charts/employment-situation/employment-levels-by-industry.htm&quot;&gt;data from BLS&lt;/a&gt; and compare the effects of 2008 recession and the ongoing pandemic on the state of employment in the U.S. across different sectors.&lt;/p&gt;

&lt;p&gt;The covid-19 pandemic has definitely wrecked the employment across several sectors of the labor economy and not just the hospitality sector. If we look at the employment numbers for an year starting from January 2020, job losses are evident. We also see the quick recovery (though not to January-2020 levels) in several of the sectors.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;job_cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Manufacturing Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Hospitality Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Wholesale Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;'Retail Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Transportation Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Construction Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;'Education Health Services Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Finance Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Information Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;'Mining Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Other Services Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Professional Services Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;'Utilities Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;area&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pandemic_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Month&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;job_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'simple_white'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_annotation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;yes&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/10.embed&quot; height=&quot;700&quot; width=&quot;980&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;If we zoom out in the data set and look at two decades of employment numbers all the way from January 2001 until January 2021, we also see the effect of 2008 recession on employment numbers. We also see how gradual and prolonged the effect of 2008 recession was. The job losses across sectors during the pandemic was sudden and steep and the recovery of numbers was also quick.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;area&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Month&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;job_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                   &lt;span class=&quot;n&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'simple_white'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_annotation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;yes&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/12.embed&quot; height=&quot;700&quot; width=&quot;980&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;We can just hope that the recovery from the pandemic continues quicker and reaches early 2020 levels, not only in the U.S. but across the globe. Given that the world is very globalized today than ever before, anyone is healthy and good only when everyone is healthy and good.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 70%&quot;&gt;
&lt;i&gt;The tabular BLS data was parsed using BeautifulSoup, and the plots were done using plotly and chart_studio.&lt;/i&gt;
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="US" />
      
        <category term="labor market" />
      
        <category term="economy" />
      
        <category term="pandemic" />
      

      

      
        <summary type="html">In a previous post, we saw how many jobs the different states in the U.S. have lost over the years primarily due to trade deficits. In this post, we will visually analyze the data from BLS and compare the effects of 2008 recession and the ongoing pandemic on the state of employment in the U.S. across different sectors. The covid-19 pandemic has definitely wrecked the employment across several sectors of the labor economy and not just the hospitality sector. If we look at the employment numbers for an year starting from January 2020, job losses are evident. We also see the quick recovery (though not to January-2020 levels) in several of the sectors. job_cols = ['Manufacturing Jobs', 'Hospitality Jobs', 'Wholesale Jobs', 'Retail Jobs', 'Transportation Jobs', 'Construction Jobs', 'Education Health Services Jobs', 'Finance Jobs', 'Information Jobs', 'Mining Jobs', 'Other Services Jobs', 'Professional Services Jobs', 'Utilities Jobs'] fig = px.area(pandemic_df, x=&quot;Month&quot;, y=job_cols, template='simple_white') fig.add_annotation(...) ... fig.show() If we zoom out in the data set and look at two decades of employment numbers all the way from January 2001 until January 2021, we also see the effect of 2008 recession on employment numbers. We also see how gradual and prolonged the effect of 2008 recession was. The job losses across sectors during the pandemic was sudden and steep and the recovery of numbers was also quick. fig = px.area(emp_df, x=&quot;Month&quot;, y=job_cols, template='simple_white') fig.add_annotation(...) ... fig.show() We can just hope that the recovery from the pandemic continues quicker and reaches early 2020 levels, not only in the U.S. but across the globe. Given that the world is very globalized today than ever before, anyone is healthy and good only when everyone is healthy and good. The tabular BLS data was parsed using BeautifulSoup, and the plots were done using plotly and chart_studio.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Jobs, Offshoring and Unemployment - US →</title>
      
      <link href="https://dotkay.github.io/2021/02/21/us-unemployment/" rel="alternate" type="text/html" title="Jobs, Offshoring and Unemployment - US" />
      <published>2021-02-21T00:00:00+00:00</published>
      <updated>2021-02-21T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/02/21/us-unemployment</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/02/21/us-unemployment/">&lt;p&gt;With 2020 being one of the worst in recent history, for many reasons fuelled by the pandemic. With many countries being unprepared to handle a pandemic of this size, it also exposed the global supply chain being &lt;a href=&quot;https://dotkay.github.io/2020/04/01/efficiency-vs-flexibility&quot;&gt;highly optimized rather than flexible&lt;/a&gt;. It also exposed the inequality in society - healthcare, job security, income, etc. Many countries had to start manufacturing personal protective equipments (PPE), ventilators, etc. by themselves to serve the local needs rather than rely on the supply chain that got stalled. A couple of decades ago, many of these equipments were manufactured locally but over time, many of those got offshored.&lt;/p&gt;

&lt;p&gt;However, this post is not about unemployment due to the pandemic. It is more about loss of U.S. jobs over the period 2001-2017, mostly due to trade deficits, with data from &lt;a href=&quot;https://www.epi.org/publication/the-china-toll-deepens-growth-in-the-bilateral-trade-deficit-between-2001-and-2017-cost-3-4-million-u-s-jobs-with-losses-in-every-state-and-congressional-district/&quot;&gt;this report&lt;/a&gt; and manufacturing data from &lt;a href=&quot;https://www.bls.gov/charts/employment-situation/employment-levels-by-industry.htm&quot;&gt;BLS data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first decade of the century was the worst for U.S. manufacturing jobs. It saw the steepest decline in such jobs, in a very long time. There are two conflicting theories around this - one argues the loss is due to increased automation, while the other attributes the loss to offshoring of such jobs. I could not find the data on rate of automation or increase in manufacturing jobs in other geographies during the time period in order to analyze it analytically. However the loss of jobs, whatever being the root cause, is itself staggering.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.express&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Month'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Manufacturing Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Manufacturing Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Manufacturing Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Manufacturing Jobs (in thousands)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'simple_white'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;autosize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;700&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;700&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;yes&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/6.embed&quot; height=&quot;700&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Apparently, about four decades ago the Bay Area in California was full of orange orchards, and a couple of decades later they all became startups working in electronics and software (Silicon Valley). They were not startups making apps that let teenagers poke and swipe at each other but advanced electronics and software that goes along with it, like routers, modems, semiconductor chips, etc. There were supposedly many small enterprises manufacturing circuit boards and assembling systems for these famous companies.&lt;/p&gt;

&lt;p&gt;An old video of Macintosh Factory in Fremont:
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Dk306ZkNOuc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;But does it mean only the electronics jobs got offshored? Not really. If we look at the data deeper and look at the number of jobs lost state-wise, it gives a more detailed picture. Almost every major state in the U.S. has lost thousands of jobs from 2001-2017, with most of the loss happening between 2001-2009. Many other states would have seen loss of jobs in other industries like automobiles and steel included.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'State'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Jobs Lost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'% Jobs Lost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Jobs Lost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Jobs Lost (in thousands)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;yes&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/1.embed&quot; height=&quot;600&quot; width=&quot;980&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;If we look even deeper into the data, we can see how many jobs were lost in each state’s congressional districts.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Total Jobs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'State'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'% Jobs Lost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'State'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'States and Congressional Districts'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;hover_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Congressional District'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;iframe id=&quot;igraph&quot; scrolling=&quot;yes&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~kkplot/4.embed&quot; height=&quot;870&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;When I find reliable data on rate of automation and manufacturing jobs gained in other geographies, analysis of that data would help in putting the pieces together. While I am not against offshoring, I am all for a more flexible supply chain, rather than a super-optimized supply chain that would get disrupted affecting the lives and livelihoods of several people, as we witnessed last year when medical devices and PPE supply chain got disrupted.&lt;/p&gt;

&lt;p&gt;While many jobs have been offshored causing large scale unemployment, there is still hope for reskilling of workers and creating new jobs that would drive the economy the next two decades - like solar panels, &lt;a href=&quot;https://dotkay.github.io/2020/09/13/electric-batteries&quot;&gt;electric batteries&lt;/a&gt;, recycling of batteries, advanced technologies for water, air purification, education and healthcare, etc.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 70%&quot;&gt;
&lt;i&gt;The tabular data from EPI report was parsed using BeautifulSoup, and the plots were done using plotly and chart_studio.&lt;/i&gt;
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="US" />
      
        <category term="labor market" />
      
        <category term="economy" />
      

      

      
        <summary type="html">With 2020 being one of the worst in recent history, for many reasons fuelled by the pandemic. With many countries being unprepared to handle a pandemic of this size, it also exposed the global supply chain being highly optimized rather than flexible. It also exposed the inequality in society - healthcare, job security, income, etc. Many countries had to start manufacturing personal protective equipments (PPE), ventilators, etc. by themselves to serve the local needs rather than rely on the supply chain that got stalled. A couple of decades ago, many of these equipments were manufactured locally but over time, many of those got offshored. However, this post is not about unemployment due to the pandemic. It is more about loss of U.S. jobs over the period 2001-2017, mostly due to trade deficits, with data from this report and manufacturing data from BLS data. The first decade of the century was the worst for U.S. manufacturing jobs. It saw the steepest decline in such jobs, in a very long time. There are two conflicting theories around this - one argues the loss is due to increased automation, while the other attributes the loss to offshoring of such jobs. I could not find the data on rate of automation or increase in manufacturing jobs in other geographies during the time period in order to analyze it analytically. However the loss of jobs, whatever being the root cause, is itself staggering. import plotly.express as px fig = px.bar(emp_df[:100], x='Month', y='Manufacturing Jobs', height=600, color='Manufacturing Jobs', labels=dict({'Manufacturing Jobs':'Manufacturing Jobs (in thousands)'}), template='simple_white') fig.update_layout( autosize=False, width=700, height=700 ) fig.show() Apparently, about four decades ago the Bay Area in California was full of orange orchards, and a couple of decades later they all became startups working in electronics and software (Silicon Valley). They were not startups making apps that let teenagers poke and swipe at each other but advanced electronics and software that goes along with it, like routers, modems, semiconductor chips, etc. There were supposedly many small enterprises manufacturing circuit boards and assembling systems for these famous companies. An old video of Macintosh Factory in Fremont: But does it mean only the electronics jobs got offshored? Not really. If we look at the data deeper and look at the number of jobs lost state-wise, it gives a more detailed picture. Almost every major state in the U.S. has lost thousands of jobs from 2001-2017, with most of the loss happening between 2001-2009. Many other states would have seen loss of jobs in other industries like automobiles and steel included. fig = px.bar(df, x='State', y='Jobs Lost', height=600, color='% Jobs Lost', labels=dict({'Jobs Lost':'Jobs Lost (in thousands)'})) fig.show() If we look even deeper into the data, we can see how many jobs were lost in each state’s congressional districts. fig = px.bar(df, x='Total Jobs', y='State', color='% Jobs Lost', labels=dict({'State':'States and Congressional Districts'}), hover_name='Congressional District') fig.show() When I find reliable data on rate of automation and manufacturing jobs gained in other geographies, analysis of that data would help in putting the pieces together. While I am not against offshoring, I am all for a more flexible supply chain, rather than a super-optimized supply chain that would get disrupted affecting the lives and livelihoods of several people, as we witnessed last year when medical devices and PPE supply chain got disrupted. While many jobs have been offshored causing large scale unemployment, there is still hope for reskilling of workers and creating new jobs that would drive the economy the next two decades - like solar panels, electric batteries, recycling of batteries, advanced technologies for water, air purification, education and healthcare, etc. The tabular data from EPI report was parsed using BeautifulSoup, and the plots were done using plotly and chart_studio.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Is it GameStop vs Hedge Funds or GameStop vs Streaming? →</title>
      
      <link href="https://dotkay.github.io/2021/01/19/gme-wall-street/" rel="alternate" type="text/html" title="Is it GameStop vs Hedge Funds or GameStop vs Streaming?" />
      <published>2021-01-19T00:00:00+00:00</published>
      <updated>2021-01-19T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2021/01/19/gme-wall-street</id>
      <content type="html" xml:base="https://dotkay.github.io/2021/01/19/gme-wall-street/">&lt;p&gt;Everyone knows GameStop now, and its ticker symbol too - gamers, non-gamers, traders, non-traders, all alike.&lt;/p&gt;

&lt;p&gt;GameStop is a chain of retail stores that sells game merchandise, equipment and at one point (when games were sold on CDs) games. Retail is moving online and if the product is software, it has already moved. I can’t recollect the last time I saw Windows OS or Office sold in a CD.&lt;/p&gt;

&lt;p&gt;Hedge funds apparently make money by betting on such companies - borrow money or company shares from some lender promising to repay and in the mean time, bet on the downside of the company, wait for the price to crash, buy the stock (for cheap) and give it back to the lender (pocketing the difference). I am not sure how they pick companies to bet on. However, late last year and early January, GameStop was the candidate. In order to revolt againt this, some online enthusiasts mobilized themselves to buy options on this stock, thereby increasing the stock price (‘stock price’ not ‘company value’) that would hurt the Hedge funds’ intentions.&lt;/p&gt;

&lt;p&gt;Let us look at some data to see if Hedge funds are the real detterants to GameStop’s future or if there are other factors - like streaming. It was not just GameStop (GME), but also Nokia (NOK), AMC and Blackberry (BB) that went through this early January, but GameStop had the biggest change. Just to show the trend in these stocks (GameStop removed from the plot below as the change in GME was too high that the other trends wouldn’t show up well). Notice how the overall trend in stock prices have been over the last few years and how it changed towards the end of 2020 and early 2021.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amc_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amc_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adj Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AMC'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nok_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nok_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adj Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'NOK'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bb_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bb_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adj Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'BB'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/amc_nok_bb.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In order to see what (game) streaming industry is like, I started looking for data on this - number of subscribers, streamers, etc. I found &lt;a href=&quot;https://twitchtracker.com/statistics&quot;&gt;Twitchtracker&lt;/a&gt; that tracks statistics on Twitch. Since Twitch’s data was easiest to get and well organized, I decided to look at just Twitch data (number of concurrent viewers) and plot it alongside GameStop stock price over the years.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twinx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gme_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'month'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gme_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adj Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;line2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twitch_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'month'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;twitch_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'avg_concurrent_viewers'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'green'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/gme_vs_twitch.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The number of Twitch channels also keep increasing along with the number of viewers and this trend is only likely to continue during these pandemic times.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'avg_concurrent_viewers'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;               
                     &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'avg_concurrent_channels'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'time_watched'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/twitch_data.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;On the other hand &lt;a href=&quot;https://www.theverge.com/2020/3/20/21188799/gamestop-california-store-closures-indefinite-coronavirus&quot;&gt;many retail stores like GameStop’s were hit hard&lt;/a&gt; during the pandemic.&lt;/p&gt;

&lt;p&gt;Everyone can infer the trend in the industry. Hedge funds are disliked for several reasons, some of them valid. But the future of gaming is online and if GameStop were to survive, it has to build that future than anchor itself in its glorious past.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 70%&quot;&gt;
&lt;i&gt;The data from Twitchtracker was parsed using BeautifulSoup, and the plots were done using matplotlib, seaborn and plotly.&lt;/i&gt;
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="GameStop" />
      
        <category term="markets" />
      
        <category term="short selling" />
      
        <category term="wall street" />
      

      

      
        <summary type="html">Everyone knows GameStop now, and its ticker symbol too - gamers, non-gamers, traders, non-traders, all alike. GameStop is a chain of retail stores that sells game merchandise, equipment and at one point (when games were sold on CDs) games. Retail is moving online and if the product is software, it has already moved. I can’t recollect the last time I saw Windows OS or Office sold in a CD. Hedge funds apparently make money by betting on such companies - borrow money or company shares from some lender promising to repay and in the mean time, bet on the downside of the company, wait for the price to crash, buy the stock (for cheap) and give it back to the lender (pocketing the difference). I am not sure how they pick companies to bet on. However, late last year and early January, GameStop was the candidate. In order to revolt againt this, some online enthusiasts mobilized themselves to buy options on this stock, thereby increasing the stock price (‘stock price’ not ‘company value’) that would hurt the Hedge funds’ intentions. Let us look at some data to see if Hedge funds are the real detterants to GameStop’s future or if there are other factors - like streaming. It was not just GameStop (GME), but also Nokia (NOK), AMC and Blackberry (BB) that went through this early January, but GameStop had the biggest change. Just to show the trend in these stocks (GameStop removed from the plot below as the change in GME was too high that the other trends wouldn’t show up well). Notice how the overall trend in stock prices have been over the last few years and how it changed towards the end of 2020 and early 2021. ... plt.plot(amc_df['Date'], amc_df['Adj Close'], label='AMC') plt.plot(nok_df['Date'], nok_df['Adj Close'], label='NOK') plt.plot(bb_df['Date'], bb_df['Adj Close'], label='BB') plt.plot() ... In order to see what (game) streaming industry is like, I started looking for data on this - number of subscribers, streamers, etc. I found Twitchtracker that tracks statistics on Twitch. Since Twitch’s data was easiest to get and well organized, I decided to look at just Twitch data (number of concurrent viewers) and plot it alongside GameStop stock price over the years. fig, ax1 = plt.subplots() ax2 = ax1.twinx() line1, = ax1.plot(gme_df['month'], gme_df['Adj Close'], color='blue') line2, = ax2.plot(twitch_df['month'], twitch_df['avg_concurrent_viewers']//1000, color='green') ... plt.show() The number of Twitch channels also keep increasing along with the number of viewers and this trend is only likely to continue during these pandemic times. fig = px.scatter(df, x='avg_concurrent_viewers', y='avg_concurrent_channels', size='time_watched') fig.show() On the other hand many retail stores like GameStop’s were hit hard during the pandemic. Everyone can infer the trend in the industry. Hedge funds are disliked for several reasons, some of them valid. But the future of gaming is online and if GameStop were to survive, it has to build that future than anchor itself in its glorious past. The data from Twitchtracker was parsed using BeautifulSoup, and the plots were done using matplotlib, seaborn and plotly.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">TSLA joins S&amp;amp;P 500 →</title>
      
      <link href="https://dotkay.github.io/2020/12/24/tsla-spy/" rel="alternate" type="text/html" title="TSLA joins S&amp;P 500" />
      <published>2020-12-24T00:00:00+00:00</published>
      <updated>2020-12-24T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2020/12/24/tsla-spy</id>
      <content type="html" xml:base="https://dotkay.github.io/2020/12/24/tsla-spy/">&lt;p&gt;A week ago Tesla Motors (TSLA) joined the S&amp;amp;P 500 Index - the stock market index that keeps track of the performance of 500 largest companies listed in stock markets in the US. Overall TSLA has been very volatile - many argue that the volatility (risk) indicates the opportunies for Tesla. Now that we are towards the end of the year, a pandemic year, let’s see how the S&amp;amp;P 500 performed and how Tesla performed.&lt;/p&gt;

&lt;p&gt;And in terms of the value of TSLA stock and S&amp;amp;P 500 Index, this is how they fared during this year:&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;img_container&quot;&gt;
  &lt;center&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dotkay/data_science/master/plots/tsla_gspc_2020.png&quot; /&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tsla_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_data_yahoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TSLA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                              &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2010-07-01&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                              &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2020-12-20&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                              &lt;span class=&quot;n&quot;&gt;interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;wk&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;snp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;snp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adj Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'S&amp;amp;P'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tsla_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tsla_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adj Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'TSLA'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Market Value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;                       
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All the data was collected using pandas_datareader and the plots done using seaborn library.&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="energy" />
      
        <category term="electric" />
      
        <category term="S&amp;P 500" />
      
        <category term="TSLA" />
      

      

      
        <summary type="html">A week ago Tesla Motors (TSLA) joined the S&amp;amp;P 500 Index - the stock market index that keeps track of the performance of 500 largest companies listed in stock markets in the US. Overall TSLA has been very volatile - many argue that the volatility (risk) indicates the opportunies for Tesla. Now that we are towards the end of the year, a pandemic year, let’s see how the S&amp;amp;P 500 performed and how Tesla performed. And in terms of the value of TSLA stock and S&amp;amp;P 500 Index, this is how they fared during this year: tsla_df = pdr.get_data_yahoo(&quot;TSLA&quot;, start=&quot;2010-07-01&quot;, end=&quot;2020-12-20&quot;, interval=&quot;wk&quot;) ... ... plt.plot(snp_df['Date'], snp_df['Adj Close'],label='S&amp;amp;P') plt.plot(tsla_df['Date'], tsla_df['Adj Close'], label='TSLA') plt.xlabel('Date') plt.ylabel('Market Value') ... ... All the data was collected using pandas_datareader and the plots done using seaborn library.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Humans and Space →</title>
      
      <link href="https://dotkay.github.io/2020/11/11/humans-and-space/" rel="alternate" type="text/html" title="Humans and Space" />
      <published>2020-11-11T00:00:00+00:00</published>
      <updated>2020-11-11T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2020/11/11/humans-and-space</id>
      <content type="html" xml:base="https://dotkay.github.io/2020/11/11/humans-and-space/">&lt;p&gt;Since time immmorial, humans have been interested in astronomy and space. This can be dated back to the &lt;a href=&quot;https://www.youtube.com/watch?v=jgjcy04PDRM&quot;&gt;work of Aryabhatta&lt;/a&gt; and the ideas of &lt;a href=&quot;https://www.youtube.com/watch?v=BMPaq8sf8Ko&quot;&gt;Greek astronomers&lt;/a&gt; like Aristotle. However, human journey to space, as we know today, happened much later. Today, we are in a race to conquer the planets and use our knowledge of space acquired over centuries to advance many technologies.&lt;/p&gt;

&lt;p&gt;Space is an important frontier for many technological advances. Global Positioning Systems (GPS), International Space Station (ISS), communication satellites, and (hopefully) very soon low earth orbit satellites beaming wireless internet are some of the greatest engineering marvels. Beyond space tourism and inhabiting Mars, there are many more important reasons why space might be very important.&lt;/p&gt;

&lt;p&gt;This fun post shows the number of trips made by American and Russian astronauts and the various expeditions to space over the last two decades.&lt;/p&gt;

&lt;iframe seamless=&quot;&quot; frameborder=&quot;0&quot; src=&quot;https://public.tableau.com/views/HumansandSpaceMissions/Dashboard1?:language=en-US&amp;amp;:display_count=n&amp;amp;:origin=viz_share_link?embed=yes&amp;amp;:display_count=yes&amp;amp;:showVizHome=no&quot; width=&quot;1100&quot; height=&quot;950&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;</content>

      
      
      
      
      

      

      
        <category term="space" />
      
        <category term="technology" />
      
        <category term="USA" />
      
        <category term="Russia" />
      

      

      
        <summary type="html">Since time immmorial, humans have been interested in astronomy and space. This can be dated back to the work of Aryabhatta and the ideas of Greek astronomers like Aristotle. However, human journey to space, as we know today, happened much later. Today, we are in a race to conquer the planets and use our knowledge of space acquired over centuries to advance many technologies. Space is an important frontier for many technological advances. Global Positioning Systems (GPS), International Space Station (ISS), communication satellites, and (hopefully) very soon low earth orbit satellites beaming wireless internet are some of the greatest engineering marvels. Beyond space tourism and inhabiting Mars, there are many more important reasons why space might be very important. This fun post shows the number of trips made by American and Russian astronauts and the various expeditions to space over the last two decades.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Our Sense of Self →</title>
      
      <link href="https://dotkay.github.io/2020/10/10/sense-of-self/" rel="alternate" type="text/html" title="Our Sense of Self" />
      <published>2020-10-10T00:00:00+00:00</published>
      <updated>2020-10-10T00:00:00+00:00</updated>
      <id>https://dotkay.github.io/2020/10/10/sense-of-self</id>
      <content type="html" xml:base="https://dotkay.github.io/2020/10/10/sense-of-self/">&lt;p&gt;We perceive our sense of self as an amalgamation of the different sensory inputs - visual, tactile, locomotive, etc., that our brain processes and interprets. Or in other words, our sense of self is an interpretation made for us by our brain (mind) after processing several inputs. In olden days philosophers who tried to understand one’s true self found it difficult to reason about the interpretation of sense of self - are we really who we are? or are we manifestations of our mind’s interpretation of several inputs? This is a fascinating topic and lately neuroscience research is trying to understand which parts of the brain are processing and how our brain creates such interpretations.&lt;/p&gt;

&lt;p&gt;In his book, &lt;a href=&quot;https://www.amazon.com/Man-Who-Wasnt-There-Investigations/dp/0525954198&quot;&gt;The Man Who Wasn’t There&lt;/a&gt;, journalist Anil Ananthaswamy takes us through the neuroscience research on this topic. The main message of this book is that our sense of self is an interpretation of several inputs which are beyond just sensory inputs but our own physical body and society play a vital role. And in this age where Computer Scientists are working on Deep Learning as well as trying to understand the neurological pathways in order to build systems that mimic the human decision making apparatus, while that apparatus itself is not completely understood.&lt;/p&gt;

&lt;p&gt;What fascinates me about this study is how the mind’s computation of sense of self can be influenced (or cheated) by changing the mapping (think of it as lookup table entries) the mind preserves for different sensory and other inputs. In many cases, what we feel and believe our sense of self is, is just an interpretation (an entry in the table) that is fetched given the inputs. We can influence such inputs to pick up a different entry that would alter our sense of self. There is also a classic neuroscience experiment from the 90s that demonstrates how the mapping in our brain, representing our physical sense of self, can be altered within minutes. You can take a look at an example experiment below:&lt;/p&gt;

&lt;div class=&quot;img_container&quot;&gt;
  &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ASM12lpDDy0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I am intrigued about the possibility of changing our emotional sense of self and if it could be done non-invasively using technology. Such a possibility would be a big boost to curing several mental disorders just by altering the mapping in the mind, when we know how the mapping is created, where it is stored and how it is interpreted. Also, one needs to be aware of ramifications of this - can such mappings be altered by say adversarial inputs - visual or societal - in the form of social media posts or news articles. We might be destroying our own sense of self by letting our mind interpret the inputs that we consume, without us realizing the end result - thinking we are who our mind interprets (influenced interpretation/self) rather than who we really are (actual self).&lt;/p&gt;</content>

      
      
      
      
      

      

      
        <category term="mind" />
      
        <category term="psychology" />
      
        <category term="philosophy" />
      
        <category term="neuroscience" />
      
        <category term="brain" />
      

      

      
        <summary type="html">We perceive our sense of self as an amalgamation of the different sensory inputs - visual, tactile, locomotive, etc., that our brain processes and interprets. Or in other words, our sense of self is an interpretation made for us by our brain (mind) after processing several inputs. In olden days philosophers who tried to understand one’s true self found it difficult to reason about the interpretation of sense of self - are we really who we are? or are we manifestations of our mind’s interpretation of several inputs? This is a fascinating topic and lately neuroscience research is trying to understand which parts of the brain are processing and how our brain creates such interpretations. In his book, The Man Who Wasn’t There, journalist Anil Ananthaswamy takes us through the neuroscience research on this topic. The main message of this book is that our sense of self is an interpretation of several inputs which are beyond just sensory inputs but our own physical body and society play a vital role. And in this age where Computer Scientists are working on Deep Learning as well as trying to understand the neurological pathways in order to build systems that mimic the human decision making apparatus, while that apparatus itself is not completely understood. What fascinates me about this study is how the mind’s computation of sense of self can be influenced (or cheated) by changing the mapping (think of it as lookup table entries) the mind preserves for different sensory and other inputs. In many cases, what we feel and believe our sense of self is, is just an interpretation (an entry in the table) that is fetched given the inputs. We can influence such inputs to pick up a different entry that would alter our sense of self. There is also a classic neuroscience experiment from the 90s that demonstrates how the mapping in our brain, representing our physical sense of self, can be altered within minutes. You can take a look at an example experiment below: I am intrigued about the possibility of changing our emotional sense of self and if it could be done non-invasively using technology. Such a possibility would be a big boost to curing several mental disorders just by altering the mapping in the mind, when we know how the mapping is created, where it is stored and how it is interpreted. Also, one needs to be aware of ramifications of this - can such mappings be altered by say adversarial inputs - visual or societal - in the form of social media posts or news articles. We might be destroying our own sense of self by letting our mind interpret the inputs that we consume, without us realizing the end result - thinking we are who our mind interprets (influenced interpretation/self) rather than who we really are (actual self).</summary>
      

      
      
    </entry>
  
  
</feed>
