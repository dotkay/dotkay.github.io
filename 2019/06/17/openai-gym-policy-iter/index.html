<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Improving a policy</title>
  <meta name="description" content="In a previous post we evaluated a given policy $\pi$ by computing the state value function $v_{\pi}(s)$ in each of the states of the system. In this post, we will start with a random policy, use the function policy_eval() developed in the previous post to improve the random policy we started with and end up in an improved policy as a result. Recall from a previous post the intuition behind action value function. Our goal now is to do a one step lookahead and calculate the action values for different possible actions and pick the action that would fetch us the maximum action value. The following function follows the equation in the sketch above and computes action values for different possible actions. def one_step_lookahead(env, st, v, gamma=0.9): action_values = np.zeros(env.action_space.n) for a in range(env.action_space.n): for nxt_st_prob, nxt_st, reward, done in env.P[st][a]: action_values[a] += nxt_st_prob * (reward + gamma * v[nxt_st]) return action_values Here is what we are going to do: Start with a random policy Evaluate the policy (using policy_eval()) and get the state values for all the states Take one step (using one_step_lookahead()) and take the action that maximizes the action value (greedy step) and then follow our usual policy for the rest of the episode Return the updated state values (resulting from one step of greedy action) along with the policy (better policy) action_values = one_step_lookahead(env, s, v) best_a = np.argmax(action_values) action_values is a 4 element array (as there are four actions - Up, Down, Right and Left). The array indices indicate the action while the array elements indicate the action value ($q(s, a$)). argmax(action_values) would return the action that has the maximum action value. Putting it all together: def policy_iter(env, gamma=0.9, max_iter=10000): # (1) start with random policy policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n # policy eval counter policy_eval_cnt = 1 # repeat until convergence or max_iter for i in range(int(max_iter)): stop = False # (2) evaluate current policy v = policy_eval(policy, env) for s in range(env.observation_space.n): # current action according to our policy curr_a = np.argmax(policy[s]) # (3) look one step ahead and check if curr_a is the best action_values = one_step_lookahead(env, s, v) # select best action greedily from action_values from one-step lookahead best_a = np.argmax(action_values) if (curr_a != best_a): stop = False # update the policy policy[s] = np.eye(env.action_space.n)[best_a] else: # curr_a has converged to best_a stop = True policy_eval_cnt += 1 if (stop): # (4) return state values and updated policy return policy, v This is referred to as Policy Iteration as we iteratively improve the policy that we started with, evaluating the policy in each iteration.">
  
    
    <meta name="keywords" content="technology,ai,openai gym,reinforcement learning">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://dotkay.github.io/2019/06/17/openai-gym-policy-iter/">
  
  
  <link rel="alternate" type="application/rss+xml" title="" href="https://dotkay.github.io/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Improving a policy">
  <meta name="twitter:description" content="In a previous post we evaluated a given policy $\pi$ by computing the state value function $v_{\pi}(s)$ in each of the states of the system. In this post, we will start with a random policy, use th...">
  
  

  
  
  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-105390377-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    <!-- Mathjax Support 
    <a class="site-title" href="/"></a> 
    -->
    <img src="/assets/images/dotk.png" width="40" height="30">
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Posts</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline"><a href="https://dotkay.github.io/2019/06/17/openai-gym-policy-iter">Improving a policy &rarr;</a></h1>
    
    <p class="post-meta"><time datetime="2019-06-17T00:00:00+00:00" itemprop="datePublished">Jun 17, 2019</time> â€¢ 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/technology/">technology</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/ai/">ai</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/openai-gym/">openai gym</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/reinforcement-learning/">reinforcement learning</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In a <a href="https://dotkay.github.io/2019/06/04/openai-gym-policy-eval">previous post</a> we evaluated a given policy $\pi$ by computing the state value function $v_{\pi}(s)$ in each of the states of the system. In this post, we will start with a random policy, use the function <code class="language-plaintext highlighter-rouge">policy_eval()</code> developed in the previous post to improve the random policy we started with and end up in an improved policy as a result.</p>

<p>Recall from a <a href="https://dotkay.github.io/2019/04/17/reinforcement-learning-bellman-exp-qsa">previous post</a> the intuition behind <em>action value</em> function.</p>

<p><br /></p>
<div class="img_container">
  <center><img src="https://raw.githubusercontent.com/dotkay/tmp/main/rl_illustrations/qsa_2.png" width="400" /></center>
</div>

<p>Our goal now is to do a one step lookahead and calculate the action values for different possible actions and pick the action that would fetch us the maximum action value. The following function follows the equation in the sketch above and computes action values for different possible actions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">one_step_lookahead</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">st</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">nxt_st_prob</span><span class="p">,</span> <span class="n">nxt_st</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">st</span><span class="p">][</span><span class="n">a</span><span class="p">]:</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">nxt_st_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">nxt_st</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">action_values</span>
</code></pre></div></div>

<p>Here is what we are going to do:</p>

<ol>
  <li>Start with a random policy</li>
  <li>Evaluate the policy (using <code class="language-plaintext highlighter-rouge">policy_eval()</code>) and get the state values for all the states</li>
  <li>Take one step (using <code class="language-plaintext highlighter-rouge">one_step_lookahead()</code>) and take the action that maximizes the action value (greedy step) and then follow our usual policy for the rest of the episode</li>
  <li>Return the updated state values (resulting from one step of greedy action) along with the policy (better policy)</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">action_values</span> <span class="o">=</span> <span class="n">one_step_lookahead</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">best_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">action_values</code> is a 4 element array (as there are four actions - Up, Down, Right and Left). The array indices indicate the action while the array elements indicate the <em>action value</em> ($q(s, a$)). <code class="language-plaintext highlighter-rouge">argmax(action_values)</code> would return the action that has the maximum <em>action value</em>.</p>

<p>Putting it all together:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>    
    <span class="c1"># (1)  start with random policy
</span>    <span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">])</span> <span class="o">/</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
    <span class="c1"># policy eval counter
</span>    <span class="n">policy_eval_cnt</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># repeat until convergence or max_iter
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)):</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="c1"># (2) evaluate current policy
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">policy_eval</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># current action according to our policy
</span>            <span class="n">curr_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
            <span class="c1"># (3) look one step ahead and check if curr_a is the best
</span>            <span class="n">action_values</span> <span class="o">=</span> <span class="n">one_step_lookahead</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="c1"># select best action greedily from action_values from one-step lookahead
</span>            <span class="n">best_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">curr_a</span> <span class="o">!=</span> <span class="n">best_a</span><span class="p">):</span>
                <span class="n">stop</span> <span class="o">=</span> <span class="bp">False</span>
                <span class="c1"># update the policy
</span>                <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)[</span><span class="n">best_a</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span> <span class="c1"># curr_a has converged to best_a
</span>                <span class="n">stop</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">policy_eval_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">stop</span><span class="p">):</span>
            <span class="c1"># (4)  return state values and updated policy
</span>            <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">v</span>
</code></pre></div></div>

<p>This is referred to as <em>Policy Iteration</em> as we iteratively improve the policy that we started with, evaluating the policy in each iteration.</p>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://dotkay.github.io/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
